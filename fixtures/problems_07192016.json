[{"model": "wordclouds.problem", "pk": 1, "fields": {"turk_id": "0IZCE0iqHL", "abstract": "Metasurfaces utilizing engineered metallic nanostructures have recently emerged as an important means to manipulate the propagation of light waves in a prescribed manner. However, conventional metallic metasurfaces mainly efficiently work in the visible and near-infrared regime, and lack sufficient tunability. In this work, combining the pronounced plasmonic resonance of patterned graphene structures with a subwavelength-thick optical cavity, we propose and demonstrate novel graphene metasurfaces that manifest the potential to dynamically control the phase and amplitude of infrared light with very high efficiency. It is shown that the phase of the infrared light reflected from a simple graphene ribbon metasurface can span over almost the entire 2 range by changing the width of the graphene ribbons, while the amplitude of the reflection can be maintained at high values without significant variations. We successfully realize anomalous reflection, reflective focusing lenses, and non-diffracting Airy beams based on graphene metasurfaces. Our results open up a new paradigm of highly integrated photonic platforms for dynamic beam shaping and adaptive optics in the crucial infrared wavelength range.", "desc": "dynamically control the phase and amplitude of infrared light with very high efficiency", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 2, "fields": {"turk_id": "0c3xFNeP5P", "abstract": "The valuation of the real option to store liquefied natural gas (LNG) at the downstream terminal of an LNG value chain is an important problem in practice. As the exact valuation of this real option is computationally intractable, we develop a novel and tractable heuristic model for its strategic valuation that integrates models of LNG shipping, natural gas price evolution, and inventory control and sale into the wholesale natural gas market. We incorporate real and estimated data to quantify the value of this real option and its dependence on the throughput of an LNG chain, the type of price variability, the type of inventory control policy employed, and the level of stochastic variability in both the shipping model and the natural gas price model used. In addition, we develop an imperfect information dual upper bound to assess the effectiveness of our heuristic, and find that our method is highly accurate. Our approach also has potential relevance to value the real option to store other commodities in facilities located downstream from a commodity production or transportation stage, such as petroleum and agricultural products, chemicals, and metals, or the real option to store the input used in the production of a commodity, such as electricity.", "desc": "valuation of liquefied natural gas storage options", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 3, "fields": {"turk_id": "1ewbCznf4F", "abstract": "History event related knowledge is precious and imagery is a powerful medium that records diverse information about the event. In this paper, we propose to automatically construct an image profile given a one sentence description of the historic event which contains where, when, who and what elements. Such a simple input requirement makes our solution easy to scale up and support a wide range of culture preservation and curation related applications ranging from wikipedia enrichment to history education. However, history relevant information on the web is available as \"wild and dirty\" data, which is quite different from clean, manually curated and structured information sources. There are two major challenges to build our proposed image profiles: 1) unconstrained image genre diversity. We categorize images into genres of documents/maps, paintings or photos. Image genre classification involves a full-spectrum of features from low-level color to high-level semantic concepts. 2) image content diversity. It can include faces, objects and scenes. Furthermore, even within the same event, the views and subjects of images are diverse and correspond to different facets of the event. To solve this challenge, we group images at two levels of granularity: iconic image grouping and facet image grouping. These require different types of features and analysis from near exact matching to soft semantic similarity. We develop a full-range feature analysis module which is composed of several levels, each suitable for different types of image analysis tasks. The wide range of features are based on both classical hand-crafted features and different layers of a convolutional neural network. We compare and study the performance of the different levels in the full-range features and show their effectiveness on handling such a wild, unconstrained dataset.", "desc": "automatically construct an image profile given a one sentence description of the historic event which contains where, when, who and what elements", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 4, "fields": {"turk_id": "1p63yA50ad", "abstract": "We describe a method to extract tabular data from web pages. Rather than just analyzing the DOM tree, we also exploit visual cues in the rendered version of the document to extract data from tables which are not explicitly marked with an HTML table element. To detect tables, we rely on a variant of the well-known X-Y cut algorithm as used in the OCR community. We implemented the system by directly accessing Mozillas box model that contains the positional data for all HTML elements of a given web page.", "desc": "extracting tabular data from web pages", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 5, "fields": {"turk_id": "25qYNuAljG", "abstract": "Shared information displays are increasingly present in built environments. Terminal displays in airports show arrival and departure information, monitors in hotels and convention centers show room assignments, and whiteboards in hospitals show schedules and help staff know what others are doing. One of the most important types of displays is the schedule board for surgical suites. Surgical suites are a highly dynamic setting, where doctors, nurses, equipment, rooms, and patients must be perfectly coordinated. Schedule changes occur frequently and must be shared among staff. This research examines the design of hospital architecture (placement of walls, corridors, furnishings) and information artifacts for more effective information sharing and coordination of surgeries. I conducted field studies in four hospital surgical suites and a survey of surgical suite directors nationwide. I describe factors of the architecture, and information available around surgical suite schedule displays that are associated with information sharing and coordination outcomes. From the field studies, I developed the concept of an information hotspot  a place where people congregate to receive and provide information, public displays offer up-to-date information, and coordination workers answer questions, resolve conflicts, and keep information up-to-date. The information hotspot concept guided my design research. I developed design principles for the placement of schedule boards and control desks; design guidelines for the location of surgical suite displays and control desks; an evaluation list for surgical suites; and a three-tiered design intervention strategy ranging in implementation effort. In a follow-up national survey of surgical suite directors, I studied linkages between surgical suite architecture, information artifacts and communication practices, workplace characteristics, information sharing, and coordination speed and stress. I found that visibility between the schedule board and control desk in the surgical suite, traffic-free areas around the schedule board, and complete, up-to-date schedule board information were related to information sharing and coordination outcomes.", "desc": "Effective information sharing and coordination of surgeries.", "user": null, "trial": null, "type": "understand + make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 6, "fields": {"turk_id": "29anwl7KZ1", "abstract": "Successful object recognition is essential for finding food, identifying kin, and avoiding danger, as well as many other adaptive behaviors. To accomplish this feat, the visual system must reconstruct 3-D interpretations from 2-D snapshots falling on the retina. Theories of recognition address this process by focusing on the question of how object representations are encoded with respect to viewpoint. Although empirical evidence has been equivocal on this question, a growing body of surprising results, including those obtained in the experiments presented in this case study, indicates that recognition is often viewpoint dependent. Such findings reveal a prominent role for viewpointdependent mechanisms and provide support for the multiple-views approach, in which objects are encoded as a set of view-specific representations that are matched to percepts using normalization procedures.", "desc": "Reconstruct 3-D interpretations from 2-D snapshots falling on the retina.", "user": null, "trial": null, "type": "understand", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 7, "fields": {"turk_id": "44SWnb2Pwl", "abstract": "This paper presents a new method for fabricating soft and stretchable liquid-phase microelectronics that feature circuit elements with micron-scale line width. In contrast to conventional microelectronics, these circuits are composed of a soft elastomer embedded with microfl uidic channels fi lled with eutectic Gallium-Indium (EGaIn) metal alloy. The EGaIn traces are liquid at room temperature and therefore remain intact and electrically functional as the surrounding elastomer elastically deforms during stretching and bending. The fabrication method uses emerging techniques in soft lithography. Microchannels are molded on to the surface of poly(dimethylsiloxane) (PDMS) elastomer and fi lled with EGaIn using a micro-transfer deposition step that exploits the unique wetting properties of EGaIn in air. After sealing with an addition layer of PDMS, the liquid-fi lled channels function as stretchable circuit wires or capacitor electrodes. The presented approach allows for the creation of micron-scale circuit features with a line width (2 m) and spacing (1 m) that is an order-of-magnitude smaller than those previously demonstrated.", "desc": "fabricating soft and stretchable liquid-phase microelectronics", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 8, "fields": {"turk_id": "7TZT6eSk2C", "abstract": "We present MindMiner, a mixed-initiative interface for capturing subjective similarity measurements via a combination of new interaction techniques and machine learning algorithms. MindMiner collects qualitative, hard to express similarity measurements from users via active polling with uncertainty and example based visual constraint creation. MindMiner also formulates human prior knowledge into a set of inequalities and learns a quantitative similarity distance metric via convex optimization. In a 12-participant peer-review understanding task, we found MindMiner was easy to learn and use, and could capture users' implicit knowledge about writing performance and cluster target entities into groups that match subjects' mental models.", "desc": "collect qualitative, hard to express similarity measurements from users", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 9, "fields": {"turk_id": "8nyAgdlaZ3", "abstract": "We show that nanowire field-effect transistor (NWFET) arrays fabricated on both planar and flexible polymeric substrates can be reproducibly interfaced with spontaneously-beating embryonic chicken hearts in both planar and bent conformations. Simultaneous recordings from glass microelectrode and NWFET devices show that NWFET conductance variations are synchronized with the beating heart. The conductance change associated with beating can be tuned substantially by device sensitivity, although the voltage-calibrated signals, 46 mV, are relatively constant and typically larger than signals recorded by microelectrode arrays. Multiplexed recording from NWFET arrays yielded signal propagation times across the myocardium with high spatial resolution. The transparent and flexible NWFET chips also enable simultaneous electrical recording and optical registration of devices to heart surfaces in three-dimensional conformations not possible with planar microdevices. The capability of simultaneous optical imaging and electrical recording also could be used to register devices to a specific region of the myocardium at the cellular level, and more generally, NWFET arrays fabricated on increasingly flexible plastic and/or biopolymer substrates have the potential to become unique tools for electrical recording from other tissue/organ samples or as powerful implants.", "desc": "reproducibly interface nanowire field-effect transistor arrays with spontaneously-beating embyronic chicken hearts", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 10, "fields": {"turk_id": "DJ2ADGD5es", "abstract": "We present two projects that facilitate collective music creativity over networks. One system is a participative social music system on mobile devices. The other is a collaborative music mixing environment that adheres to the Creative Commons license [1]. We discuss how network and community infrastructures affect the creative musical process, and the implications for artists creating new content for these formats. The projects described are real-world examples of collaborative systems as musical works.", "desc": "facilitate collective music creativity over networks", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 11, "fields": {"turk_id": "nN5e35FqQt", "abstract": "How effective are call and SMS logs in modeling tie strength? Frequency and duration of communication has long been cited as a major aspect of tie strength. Intuitively, this makes sense: people communicate with those that they feel close to. Highly cited research papers have pushed this idea further, using communication as a direct proxy for tie strength. However, this operationalization has not been validated. Our work evaluates this assumption. We collected call and SMS logs and ground truth relationship data from 36 participants. Consistent with theory, we found that frequent or long-duration communication likely indicates a strong tie. However, the use of call and SMS logs produced many errors in separating strong and weak ties, suggesting this approach is incomplete. Follow-up interviews indicate fundamental challenges for inferring tie strength from communication logs.", "desc": "measure importance of call and SMS logs in modeling tie strength", "user": null, "trial": null, "type": "only findings", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 12, "fields": {"turk_id": "LXGR3Bnn9Y", "abstract": "We study the segmental recurrent neural network for end-to-end acoustic modelling. This model connects the segmental conditional random field (CRF) with a recurrent neural network (RNN) used for feature extraction. Compared to most previous CRF-based acoustic models, it does not rely on an external system to provide features or segmentation boundaries. Instead, this model marginalises out all the possible segmentations, and features are extracted from the RNN trained together with the segmental CRF. In essence, this model is self-contained and can be trained end-to-end. In this paper, we discuss practical training and decoding issues as well as the method to speed up the training in the context of speech recognition. We performed experiments on the TIMIT dataset. We achieved 17.3 phone error rate (PER) from the first-pass decoding --- the best reported result using CRFs, despite the fact that we only used a zeroth-order CRF and without using any language model.", "desc": "transcribe speech with end-to-end acoustic modelling", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 13, "fields": {"turk_id": "SJ80w7ip0u", "abstract": "Open Display Networks have the potential to allow many content creators to publish their media to an open-ended set of screen displays. However, this raises the issue of how to match that content to the right displays. In this study, we aim to understand how the perceived utility of particular media sharing scenarios is affected by three independent variables, more specifically: (a) the locativeness of the content being shared; (b) how personal that content is and (c) the scope in which it is being shared. To assess these effects, we composed a set of 24 media sharing scenarios embedded with different treatments of our three independent variables. We then asked 100 participants to express their perception of the relevance of those scenarios. The results suggest a clear preference for scenarios where content is both local and directly related to the person that is publishing it. This is in stark contrast to the types of content that are commonly found in public displays, and confirms the opportunity that open displays networks may represent a new media for self-expression. This novel understanding may inform the design of new publication paradigms that will enable people to share media across the display networks.", "desc": "How do locativeness, personalness, and scope affect percieved utility of media?", "user": null, "trial": null, "type": "only findings", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 14, "fields": {"turk_id": "eCKxu8LarU", "abstract": "We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. We explain efficient inference procedures that allow application to both parsing and language modeling. Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese.", "desc": "parse language", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 15, "fields": {"turk_id": "Ampj8Jjx6D", "abstract": "The idea of using a data-driven phoneme confusion matrix (PCM) to enhance speech recognition and retrieval performance is not new to the speech community. Although empirical results show various degrees of improvements brought by introducing a PCM, the underlying data-driven processes introduced in most papers are rather ad-hoc and lack rigorous statistical justifications. In this paper we will focus on the statistical aspects of PCM generation, propose and justify a novel expectation-maximization based algorithm for data-driven PCM generation. We will evaluate the performance of the generated PCMs under the context of low-resource spoken term detection, with primary focus on out-of-vocabulary keywords.", "desc": "transcribe speech using a data-driven phoneme confusion matrix (PCM)", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 16, "fields": {"turk_id": "DRZyPHKydh", "abstract": "We present MindMiner, a mixed-initiative interface for capturing subjective similarity measurements via a combination of new interaction techniques and machine learning algorithms. MindMiner collects qualitative, hard to express similarity measurements from users via active polling with uncertainty and example based visual constraint creation. MindMiner also formulates human prior knowledge into a set of inequalities and learns a quantitative similarity distance metric via convex optimization. In a 12-subject peer-review understanding task, we found MindMiner was easy to learn and use, and could capture users\u2030\u0170\u015e implicit knowledge about writing performance and cluster target entities into groups that match subjects\u2030\u0170\u015e mental models. We also found that MindMiner\u2030\u0170\u015es constraint suggestions and uncertainty polling functions could improve both efficiency and the quality of clustering.", "desc": "capture subjective similarity measurements", "user": null, "trial": null, "type": "similar to above", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 17, "fields": {"turk_id": "iN5LbwSuyI", "abstract": "Rapid feedback is a core component of mastery learning, but feedback on open-ended work requires days or weeks in most classes today. This paper introduces PeerStudio, an assessment platform that leverages the large number of students' peers in online classes to enable rapid feedback on in-progress work. Students submit their draft, give rubric-based feedback on two peers' drafts, and then receive peer feedback. Students can integrate the feedback and repeat this process as often as they desire. In MOOC deployments, the median student received feedback in just twenty minutes. Rapid feedback on in-progress work improves course outcomes: in a controlled experiment, students' final grades improved when feedback was delivered quickly, but not if delayed by 24 hours. More than 3,600 students have used PeerStudio in eight classes, both massive and in-person. This research demonstrates how large classes can leverage their scale to encourage mastery through rapid feedback and revision.", "desc": "give students feedback for works in progress rapidly", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 18, "fields": {"turk_id": "60abWfXMXp", "abstract": "Automatic real-time captioning provides immediate and on demand access to spoken content in lectures or talks, and is a crucial accommodation for deaf and hard of hearing (DHH) people. However, in the presence of specialized content, like in technical talks, automatic speech recognition (ASR) still makes mistakes which may render the output incomprehensible. In this paper, we introduce a new approach, which allows audience or crowd workers, to quickly correct errors that they spot in ASR output. Prior approaches required the crowd worker to manually \u2030\u0170\u010eedit\u2030\u0170\u0165 the ASR hypothesis by selecting and replacing the text, which is not suitable for real-time scenarios. Our approach is faster and allows the worker to simply type corrections for misrecognized words as soon as he or she spots them. The system then finds the most likely position for the correction in the ASR output using keyword search (KWS) and stitches the word into the ASR output. Our work demonstrates the potential of computation to incorporate human input quickly enough to be usable in real-time scenarios, and may be a better method for providing this vital accommodation to DHH people.", "desc": "transcribe speech in real time when technical vocabulary must be accurately captured", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 19, "fields": {"turk_id": "7TZT6eSk2C", "abstract": "We present MindMiner, a mixed-initiative interface for capturing subjective similarity measurements via a combination of new interaction techniques and machine learning algorithms. MindMiner collects qualitative, hard to express similarity measurements from users via active polling with uncertainty and example based visual constraint creation. MindMiner also formulates human prior knowledge into a set of inequalities and learns a quantitative similarity distance metric via convex optimization. In a 12-participant peer-review understanding task, we found MindMiner was easy to learn and use, and could capture users' implicit knowledge about writing performance and cluster target entities into groups that match subjects' mental models.", "desc": "capture subjective similarity measurements", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 20, "fields": {"turk_id": "ZXrFNYYkag", "abstract": "Many people use the Internet every day yet know little about how it really works. Prior literature diverges on how people\u2030\u0170\u015es Internet knowledge affects their privacy and security decisions. We undertook a qualitative study to understand what people do and do not know about the Internet and how that knowledge affects their responses to privacy and security risks. Lay people, as compared to those with computer science or related backgrounds, had simpler mental models that omitted Internet levels, organizations, and entities. People with more articulated technical models perceived more privacy threats, possibly driven by their more accurate understanding of where specific risks could occur in the network. Despite these differences, we did not find a direct relationship between people\u2030\u0170\u015es technical background and the actions they took to control their privacy or increase their security online. Consistent with other work on user knowledge and experience, our study suggests a greater emphasis on policies and systems that protect privacy and security without relying too much on users\u2030\u0170\u015e security practices.", "desc": "How do people's understanding of the internet affect their online security practices?", "user": null, "trial": null, "type": "moslty findings", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 21, "fields": {"turk_id": "o3p2tGLbM3", "abstract": "Telepresence means business people can make deals in other countries, doctors can give remote medical advice, and soldiers can rescue someone from thousands of miles away. When interaction is mediated, people are removed from and lack context about the person they are making decisions about. In this paper, we explore the impact of technological mediation on risk and dehumanization in decision-making. We conducted a laboratory experiment involving medical treatment decisions. The results suggest that technological mediation influences decision making, but its influence depends on an individual's self-construal: participants who saw themselves as defined through their relationships (interdependent self-construal) recommended riskier and more painful treatments in video conferencing than when face-to-face. We discuss implications of our results for theory and future research.", "desc": "what is the impact of technological mediation on risk and dehumanization in decision-making", "user": null, "trial": null, "type": "only findings", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 22, "fields": {"turk_id": "ttdFoU0dXa", "abstract": "Social influence is key in technology adoption, but its role in security-feature adoption is unique and remains unclear. Here, we analyzed how three Facebook security features' Login Approvals, Login Notifications, and Trusted Contacts-diffused through the social networks of 1.5 million people. Our results suggest that social influence affects one's likelihood to adopt a security feature, but its effect varies based on the observability of the feature, the current feature adoption rate among a potential adopter's friends, and the number of distinct social circles from which those feature-adopting friends originate. Curiously, there may be a threshold higher than which having more security feature adopting friends predicts for higher adoption likelihood, but below which having more feature-adopting friends predicts for lower adoption likelihood. Furthermore, the magnitude of this threshold is modulated by the attributes of a feature-features that are more noticeable (Login Approvals, Trusted Contacts) have lower thresholds.", "desc": "How does social influence affect technology adoption?", "user": null, "trial": null, "type": "only findings", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 23, "fields": {"turk_id": "z2bBxlX9Ax", "abstract": "Increasingly, the advice people receive on the Internet is socially transparent in the sense that it displays contextual information about the advice-givers or their actions. We hypothesize that activity transparency -seeing an advice giver's process while creating his or her recommendations - will increase advice taking. We report three experiments testing the effect of activity transparency on taking mediocre advice. We found that the presence of a web history increased the likelihood of following a financial advisor's advice and reduced participant earnings (Exp. 1), especially when the web history implied greater task focus (Exp. 2, 3). CSCW research usually emphasizes how to increase information sharing; this work suggests when shared information may be inappropriate. We suggest ways to counter activity transparency's potential downsides.", "desc": "How does transparency in advice-giving affect acceptance of advice?", "user": null, "trial": null, "type": "only findings", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 24, "fields": {"turk_id": "tUE0kJiZhF", "abstract": "In a variety of peer production settings, from Wikipedia to open source software development to crowdsourcing, individuals may encounter, edit, or review the work of unknown others. Typically this is done without much context to the person's past behavior or performance. To understand how exposure to an unknown individual's activity history influences attitudes and behaviors, we conducted an online experiment on Mechanical Turk varying the content, quality, and presentation of information about another Turker's work history. Surprisingly, negative work history did not lead to negative outcomes, but in contrast, a positive work history led to positive initial impressions that persisted in the face of contrary information. This work provides insight into the impact of activity history design factors on psychological and behavioral outcomes that can be of use in other related settings.", "desc": "How does activity history design influence psychological and behavioral outcomes?", "user": null, "trial": null, "type": "only findings", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 25, "fields": {"turk_id": "4NHNbqUpN0", "abstract": "Software algorithms are changing how people work in an ever-growing number of fields, managing distributed human workers at a large scale. In these work settings, human jobs are assigned, optimized, and evaluated through algorithms and tracked data. We explore the impact of this algorithmic, data-driven management on human workers and work practices in the context of Uber and Lyft, new ridesharing services. Our findings from a qualitative study describe how drivers responded when algorithms assigned work, provided informational support, and evaluated their performance, and how drivers used online forums to socially make sense of the algorithm features. Implications and future work are discussed.", "desc": "How are software algorithms chaning the work place?", "user": null, "trial": null, "type": "only findings", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 26, "fields": {"turk_id": "xufI07lklP", "abstract": "Large-scale content-based semantic search in video is an interesting and fundamental problem in multimedia analysis and retrieval. Existing methods index a video by the raw concept detection score that is dense and inconsistent, and thus cannot scale to \"big data\" that are readily available on the Internet. This paper proposes a scalable solution. The key is a novel step called concept adjustment that represents a video by a few salient and consistent concepts that can be efficiently indexed by the modified inverted index. The proposed adjustment model relies on a concise optimization framework with interpretations. The proposed index leverages the text-based inverted index for video retrieval. Experimental results validate the efficacy and the efficiency of the proposed method. The results show that our method can scale up the semantic search while maintaining state-of-the-art search performance. Specifically, the proposed method (with reranking) achieves the best result on the challenging TRECVID Multimedia Event Detection (MED) zero-example task. It only takes 0.2 second on a single CPU core to search a collection of 100 million Internet videos.", "desc": "search video with Large-scale content-based semantic search when scaliblility is an issue", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 27, "fields": {"turk_id": "ZjU2Sl42x2", "abstract": "Websites can record individual users' activities and display them in a variety of ways. There is a tradeoff between detail and abstraction in visualization, especially when the amount of content increases and becomes more difficult to process. We conducted an experiment on Mechanical Turk varying the quality, detail, and visual presentation of information about an individual's past work to see how these design features affected perceptions of the worker. We found that providing detail in the display through text increased processing time and led to less positive evaluations. Visually abstract displays required less processing time but decreased confidence in evaluation. This suggests that different design parameters may engender differing psychological processes that influence reactions towards an unknown person.", "desc": "how do different design parameters (detail vs abstraction) about work history affect perceptions of the worker?", "user": null, "trial": null, "type": "only findings", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 28, "fields": {"turk_id": "oH5JTKkAC3", "abstract": "Anonymity online is important to people at times in their lives. Anonymous communication applications such as Whisper and YikYak enable people to communicate with strangers anonymously through their smartphones. We report results from semi-structured interviews with 18 users of these apps. The goal of our study was to identify why and how people use anonymous apps, their perceptions of their audience and interactions on the apps, and how these apps compare with other online social communities. We present a typology of the content people share, and their motivations for participation in anonymous apps. People share various types of content that range from deep confessions and secrets to lighthearted jokes and momentary feelings. An important driver for participation and posting is to get social validation from others, even though they are anonymous strangers. We also find that participants believe these anonymous apps allow more honesty, openness, and diversity of opinion than they can find elsewhere. Our results provide implications for how anonymity in mobile apps can encourage expressiveness and interaction among users.", "desc": "identify why and how people use anonymous apps, their perceptions of their audience and interactions on the apps", "user": null, "trial": null, "type": "only findings", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 29, "fields": {"turk_id": "88AigHPOSi", "abstract": "As news reading becomes more social, how do different types of annotations affect people's selection of news articles? This paper reports on results from two experiments looking at social annotations in two different news reading contexts. The first experiment simulates a logged-out experience with annotations from strangers, a computer agent, and a branded company. Results indicate that, perhaps unsurprisingly, annotations by strangers have no persuasive effects. However, surprisingly, unknown branded companies still had a persuasive effect. The second experiment simulates a logged-in experience with annotations from friends, finding that friend annotations are both persuasive and improve user satisfaction over their article selections. In post-experiment interviews, we found that this increased satisfaction is due partly because of the context that annotations add. That is, friend annotations both help people decide what to read, and provide social context that improves engagement. Interviews also suggest subtle expertise effects. We discuss implications for design of social annotation systems and suggestions for future research.", "desc": "how do different types of annotations affect people's selection of news articles", "user": null, "trial": null, "type": "only findings", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 30, "fields": {"turk_id": "ehOfqxRrFL", "abstract": "We present an approach for the detection of coordinate-term relationships between entities from the software domain, that refer to Java classes. Usually, relations are found by examining corpus statistics associated with text entities. In some technical domains, however, we have access to additional information about the real-world objects named by the entities, suggesting that coupling information about the \"grounded\" entities with corpus statistics might lead to improved methods for relation discovery. To this end, we develop a similarity measure for Java classes using distributional information about how they are used in software, which we combine with corpus statistics on the distribution of contexts in which the classes appear in text. Using our approach, cross-validation accuracy on this dataset can be improved dramatically, from around 60% to 88%. Human labeling results show that our classifier has an F1 score of 86% over the top 1000 predicted pairs.", "desc": "detect coordinate-term relationships between entities (java classes)", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 31, "fields": {"turk_id": "0xa6LSLYdg", "abstract": "We introduce an algorithm for automatic selection of semantically-resonant colors to represent data (e.g., using blue for data about \u2030\u0170\u010eoceans\u2030\u0170\u0165, or pink for \u2030\u0170\u010elove\u2030\u0170\u0165). Given a set of categorical values and a target color palette, our algorithm matches each data value with a unique color. Values are mapped to colors by collecting representative images, analyzing image color distributions to determine value-color affinity scores, and choosing an optimal assignment. Our affinity score balances the probability of a color with how well it discriminates among data values. A controlled study shows that expert-chosen semantically-resonant colors improve speed on chart reading tasks compared to a standard palette, and that our algorithm selects colors that lead to similar gains. A second study verifies that our algorithm effectively selects colors across a variety of data categories.", "desc": "select semantically-resonant colors to represent data automatically", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 32, "fields": {"turk_id": "1ewbCznf4F", "abstract": "History event related knowledge is precious and imagery is a powerful medium that records diverse information about the event. In this paper, we propose to automatically construct an image profile given a one sentence description of the historic event which contains where, when, who and what elements. Such a simple input requirement makes our solution easy to scale up and support a wide range of culture preservation and curation related applications ranging from wikipedia enrichment to history education. However, history relevant information on the web is available as \"wild and dirty\" data, which is quite different from clean, manually curated and structured information sources. There are two major challenges to build our proposed image profiles: 1) unconstrained image genre diversity. We categorize images into genres of documents/maps, paintings or photos. Image genre classification involves a full-spectrum of features from low-level color to high-level semantic concepts. 2) image content diversity. It can include faces, objects and scenes. Furthermore, even within the same event, the views and subjects of images are diverse and correspond to different facets of the event. To solve this challenge, we group images at two levels of granularity: iconic image grouping and facet image grouping. These require different types of features and analysis from near exact matching to soft semantic similarity. We develop a full-range feature analysis module which is composed of several levels, each suitable for different types of image analysis tasks. The wide range of features are based on both classical hand-crafted features and different layers of a convolutional neural network. We compare and study the performance of the different levels in the full-range features and show their effectiveness on handling such a wild, unconstrained dataset.", "desc": "mine images in a scalable approach", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 33, "fields": {"turk_id": "jBG3Ql3irM", "abstract": "Distant labeling for information extraction (IE) suffers from noisy training data. We describe a way of reducing the noise associated with distant IE by identifying coupling constraints between potential instance labels. As one example of coupling, items in a list are likely to have the same label. A second example of coupling comes from analysis of document structure: in some corpora, sections can be identified such that items in the same section are likely to have the same label. Such sections do not exist in all corpora, but we show that augmenting a large corpus with coupling constraints from even a small, well-structured corpus can improve performance substantially, doubling F1 on one task.", "desc": "reduce the noise associated with distant information extraction", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 34, "fields": {"turk_id": "GKBibn77fm", "abstract": "Student discussions over video in massive classes allow students to explore course content, share personal experiences and get feedback on their ideas. However, such discussions frequently turn into casual conversations without focusing on the curriculum and the learning objectives. This short paper explores whether students can achieve multiple learning objectives by solving challenges collaboratively during discussions. We introduce `think-pair-share' technique for video discussions. Our pilot results, drawn from a Coursera class, suggest that participants prefer to exchange information with their peers using personal stories and connecting stories with curriculum increases participant engagement.", "desc": "solve challenges collaboratively during discussions to achieve multiple learning objectives", "user": null, "trial": null, "type": "only findings", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 35, "fields": {"turk_id": "SgtlaXeiu1", "abstract": "This study explored the association between different types of brief disagreements and subsequent levels of expressed psychological uncertainty, a fundamental cognitive aspect of complex problem solving. We examined 11\u2030\u0170\u00e4hours (11\u2030\u0170\u00e4861 utterances) of conversations in expert science teams, sampled across the first 90\u2030\u0170\u00e4days of the Mars Exploration Rover mission. Utterances were independently coded for micro-conflicts and expressed psychological uncertainty. Using time-lagged hierarchical linear modeling applied to blocks of 25 utterances, we found that micro-conflicts regarding rover planning were followed by greater uncertainty. Brief disagreements about science issues were followed by an increase in expressed uncertainty early in the mission. Examining the potential reverse temporal association, uncertainty actually predicted fewer subsequent disagreements, ruling out indirect, third variable associations of conflict and uncertainty. Overall, these findings suggest that some forms of disagreement may serve to uncover important areas of uncertainty in complex teamwork, perhaps via revealing differences in mental models.Copyright \u013a\u00a9 2016 John Wiley & Sons, Ltd.", "desc": "association between different types of brief disagreements and subsequent levels of expressed psychological uncertainty", "user": null, "trial": null, "type": "only findings", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 36, "fields": {"turk_id": "JREFuyRcGP", "abstract": "In today's systems, restricting the authority of untrusted code is difficult because, by default, code has the same authority as the user running it. Object capabilities are a promising way to implement the principle of least authority, but being too low-level and fine-grained, take away many conveniences provided by module systems. We present a module system design that is capability-safe, yet preserves most of the convenience of conventional module systems. We demonstrate how to ensure key security and privacy properties of a program as a mode of use of our module system. Our authority safety result formally captures the role of mutable state in capability-based systems and uses a novel non-transitive notion of authority, which allows us to reason about authority restriction: the encapsulation of a stronger capability inside a weaker one.", "desc": "restrict the authority of untrusted code", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 37, "fields": {"turk_id": "Kkm3NKKW9R", "abstract": "We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the word- and sentence-level, enabling it to attend differentially to more and less important con- tent when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.", "desc": "classify documents", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 38, "fields": {"turk_id": "M4aELnB7g6", "abstract": "In this paper, we define the problem of coreference resolution in text as one of clustering with pair- wise constraints where human experts are asked to provide pairwise constraints (pairwise judgments of coreferentiality) to guide the clustering process. Positing that these pairwise judgments are easy to obtain from humans given the right context, we show that with significantly lower number of pair- wise judgments and feature-engineering effort, we can achieve competitive coreference performance. Further, we describe an active learning strategy that minimizes the overall number of such pairwise judgments needed by asking the most informative questions to human experts at each step of coreference resolution. We evaluate this hypothesis and our algorithms on both entity and event coreference tasks and on two languages.", "desc": "resolve coreferences in text", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 39, "fields": {"turk_id": "PWdaJSkSJz", "abstract": "Microblogs such as Twitter, Facebook, and Sina Weibo (China's equivalent of Twitter), are a remarkable linguistic resource. In contrast to content from edited genres such as newswire, microblogs contain discussions of virtually every topic by numerous individuals in different languages and dialects and in different styles. In this work, we show that some microblog users post \u2030\u0170\u010eself-translated\u2030\u0170\u0165 messages targeting audiences who speak different languages, either by writing the same message in multiple languages or by retweeting translations of their original posts in a second language. We introduce a method for finding and extracting this naturally occurring parallel data. Identifying the parallel content requires solving an alignment problem, and we give an optimally efficient dynamic programming algorithm for this. Using our method, we extract nearly 3M Chinese\u2030\u0170\u0147English parallel segments from Sina Weibo using a targeted crawl of Weibo users who post in multiple languages. Additionally, from a random sample of Twitter, we are obtain substantial amounts of parallel data in multiple language pairs. Evaluation is performed by assessing the accuracy of our extraction approach relative to a manual annotation as well as in terms of utility as training data for a Chinese\u2030\u0170\u0147English machine translation system. Relative to traditional parallel data resources, the automatically extracted parallel data yields substantial translation quality improvements in translating microblog text and modest improvements in translating edited news content.", "desc": "translate text", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 40, "fields": {"turk_id": "M428ps2Ieq", "abstract": "Words are polysemous. However, most approaches to representation learning for lexical semantics assign a single vector to every surface word type. Meanwhile, lexical ontologies such as WordNet provide a source of complementary knowledge to distributional information, including a word sense inventory. In this paper we propose two novel and general approaches for generating sense-specific word embeddings that are grounded in an ontology. The first applies graph smoothing as a postprocessing step to tease the vectors of different senses apart, and is applicable to any vector space model. The second adapts predictive maximum likelihood models that learn word embeddings with latent variables representing senses grounded in an specified ontology. Empirical results on lexical semantic tasks show that our approaches effectively captures information from both the ontology and distributional statistics. Moreover, in most cases our sense-specific models outperform other models we compare against.", "desc": "differentiate between the multiple meanings or senses of a word", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 41, "fields": {"turk_id": "ttrSED7A3K", "abstract": "Content-based medical image retrieval (CBMIR) is an active research area for disease diagnosis and treatment but it can be problematic given the small visual variations between anatomical structures. We propose a retrieval method based on a bag-of-visual-words (BoVW) to identify discriminative characteristics between different medical images with Pruned Dictionary based on Latent Semantic Topic description. We refer to this as the PD-LST retrieval. Our method has two main components. First, we calculate a topic-word significance value for each visual word given a certain latent topic to evaluate how the word is connected to this latent topic. The latent topics are learnt, based on the relationship between the images and words, and are employed to bridge the gap between low-level visual features and high-level semantics. These latent topics describe the images and words semantically and can thus facilitate more meaningful comparisons between the words. Second, we compute an overall-word significance value to evaluate the significance of a visual word within the entire dictionary. We designed an iterative ranking method to measure overall-word significance by considering the relationship between all latent topics and words. The words with higher values are considered meaningful with more significant discriminative power in differentiating medical images. We evaluated our method on two public medical imaging datasets and it showed improved retrieval accuracy and efficiency.", "desc": "content-based medical image retrieval when there are small visual variations between anatomical structures", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 42, "fields": {"turk_id": "H3gqen0wsM", "abstract": "Distant speech recognition (DSR) remains to be an open chal- lenge, even for the state-of-the-art deep neural network (DNN) models. Previous work has attempted to improve DNNs un- der constantly distant speech. However, in real applications, the speaker-microphone distance (SMD) can be quite dynamic, varying even within a single utterance. This paper investigates how to alleviate the impact of dynamic SMD on DNN models. Our solution is to incorporate the frame-level SMD information into DNN training. Generation of the SMD information relies on a universal extractor that is learned on a meeting corpus. We study the utility of different architectures in instantiating the SMD extractor. On our target acoustic modeling task, two approaches are proposed to build distance-aware DNN models using the SMD information: simple concatenation and distance adaptive training (DAT). Our experiments show that in the simplest case, incorporating the SMD descriptors improves word error rates of DNNs by 5.6% relative. Further optimizing SMD extraction and integration results in more gains.", "desc": "transcribe speech the when speaker-microphone distance is dynamic", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 43, "fields": {"turk_id": "MUt1MbjBFa", "abstract": "The pervasiveness of mobile technologies today have facilitated the creation of massive crowdsourced and geotagged data from individual users in real time and at different locations in the city. Such ubiquitous user-generated data allow us to infer various patterns of human behavior, which help us understand the interactions between humans and cities. In this study, we focus on understanding users economic behavior in the city by examining the economic value from crowdsourced and geotaggged data. Specifically, we extract multiple traffic and human mobility features from publicly available data sources using NLP and geo-mapping techniques, and examine the effects of both static and dynamic features on economic outcome of local businesses. Our study is instantiated on a unique dataset of restaurant bookings from OpenTable for 3,187 restaurants in New York City from November 2013 to March 2014. Our results suggest that foot traffic can increase local popularity and business performance, while mobility and traffic from automobiles may hurt local businesses, especially the well-established chains and high-end restaurants. We also find that on average one more street closure nearby leads to a 4.7% decrease in the probability of a restaurant being fully booked during the dinner peak. Our study demonstrates the potential of how to best make use of the large volumes and diverse sources of crowdsourced and geotagged user-generated data to create matrices to predict local economic demand in a manner that is fast, cheap, accurate, and meaningful.", "desc": "understand users economic behavior in the city", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 44, "fields": {"turk_id": "fhE2nxUl6x", "abstract": "Gaze-based interaction has several benefits: naturalism, remote controllability, and easy accessibility. However, it has been mostly used for screen-based interaction with static information. In this paper, we propose a concept of gaze-based interaction that augments the physical world with social information. We demonstrate this interaction in a shopping scenario. In-store shopping is a setting where social information can augment the physical environment to better support a user's purchase decision. Based on the user's ocular point, we project the following information on the product and its surrounding surface: collective in-store gazes and purchase data, product comparison information, animation expressing ingredient of product, and online social comments. This paper presents the design of the system, the results and discussion of an informal user study, and future work.", "desc": "improve decision-making", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 45, "fields": {"turk_id": "A676MBYRAa", "abstract": "In multi-person tracking scenarios, gaining access to the identity of each tracked individual is crucial for many applications such as long-term surveillance video analysis. Therefore, we propose a long-term multi-person tracker which utilizes face recognition information to not only enhance tracking performance, but also assign identities to tracked people. As face recognition information is not available in many frames, the proposed tracker utilizes manifold learning techniques to propagate identity information to frames without face recognition information. Our tracker is formulated as a constrained quadratic optimization problem, which is solved with nonnegative matrix optimization techniques. Tracking experiments performed on challenging data sets, including a 116.25 hour complex indoor tracking data set, showed that our method is effective in tracking each individual. We further explored the utility of long-term identity-aware multi-person tracking output by performing video summarization experiments based on our tracking output. Results showed that the computed trajectories were sufficient to generate a reasonable visual diary (i.e. a summary of what a person did) for different people, thus potentially opening the door to summarization of hundreds or even thousands of hours of surveillance video.", "desc": "identify and track individuals in long videos", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 46, "fields": {"turk_id": "er2gHc4u15", "abstract": "Modern smartphone platforms offer a multitude of useful features to their users but at the same time they are highly privacy affecting. However, smartphone platforms are not effective in properly communicating privacy risks to their users. Furthermore, common privacy risk communication approaches in smartphone app ecosystems do not consider the actual data-access behavior of individual apps in their risk assessments. Beyond privacy risks such as the leakage of single information (first-order privacy risk), we argue that privacy risk assessments and risk communication should also consider threats to user privacy coming from user-profiling and data-mining capabilities based on the long-term data-access behavior of apps (second-order privacy risk). In this paper, we introduce Styx, a novel privacy risk communication system for Android that provides users with privacy risk information based on the second-order privacy risk perspective. We discuss results from an experimental evaluation of Styx regarding its effectiveness in risk communication and its effects on user perceptions such as privacy concerns and the trustworthiness of a smartphone. Our results suggest that privacy risk information provided by Styx improves the comprehensibility of privacy risk information and helps the users in comparing different apps regarding their privacy properties. The results further suggest that an improved privacy risk communication on smartphones can increase trust towards a smartphone and reduce privacy concern.", "desc": "evaluate privacy risks to users of smartphone platforms", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 47, "fields": {"turk_id": "H1aE9Mqbdp", "abstract": "In the physical classroom, peer interactions motivate students and expand their perspective. We suggest that synchronous peer interaction can benefit massive online courses as well. Talkabout organizes students into video discussion groups and allows instructors to determine group composition and discussion content. Using Talkabout, students pick a discussion time that suits their schedule. The system groups the students into small video discussions based on instructor preferences such as gender or geographic balance. To date, 2,474 students in five massive online courses have used Talkabout to discuss topics ranging from prejudice to organizational theory. Talkabout discussions are diverse: in one course, the median six-person discussion group had students from four different countries. Students enjoyed discussing in these diverse groups: the average student participated for 66 minutes, twice the course requirement. Students in more geographically distributed groups also scored higher on the final, suggesting that distributed discussions have educational value.", "desc": "apply peer interactions to online courses", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 48, "fields": {"turk_id": "s32EU2Ddyb", "abstract": "We propose two well-motivated ranking-based methods to enhance the performance of current state-of-the-art human activity recognition systems. First, as an improvement over the classic power normalization method, we propose a parameter-free ranking technique called rank normalization (RaN). RaN normalizes each dimension of the video features to address the sparse and bursty distribution problems of Fisher Vectors and VLAD. Second, inspired by curriculum learning, we introduce a training-free re-ranking technique called multi-class iterative re-ranking (MIR). MIR captures relationships among action classes by separating easy and typical videos from difficult ones and re-ranking the prediction scores of classifiers accordingly. We demonstrate that our methods significantly improve the performance of state-of-the-art motion features on six real-world datasets.", "desc": "recognize human activity when performance needs to be enhanced", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 49, "fields": {"turk_id": "0rBGg9IEQM", "abstract": "We introduce UniAuth, a set of mechanisms for streamlining authentication to devices and web services. With UniAuth, a user first authenticates himself to his UniAuth client, typically his smartphone or wearable device. His client can then authenticate to other services on his behalf. In this paper, we focus on exploring the user experiences with an early iPhone prototype called Knock x Knock. To manage a variety of accounts securely in a usable way, Knock x Knock incorporates features not supported in existing password managers, such as tiered and location-aware lock control, authentication to laptops via knocking, and storing credentials locally while working with laptops seamlessly. In two field studies, 19 participants used Knock x Knock for one to three weeks with their own devices and accounts. Our participants were highly positive about Knock x Knock, demonstrating the desirability of our approach. We also discuss interesting edge cases and design implications.", "desc": "manage a variety of accounts securely in a usable way,", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 50, "fields": {"turk_id": "xfAn6Bzf0y", "abstract": "Multimedia event detection has been one of the major endeavors in video event analysis. A variety of approaches have been proposed recently to tackle this problem. Among others, using semantic representation has been accredited for its promising performance and desirable ability for human-understandable reasoning. To generate semantic representation, we usually utilize several external image/video archives and apply the concept detectors trained on them to the event videos. Due to the intrinsic difference of these archives, the resulted representation is presumable to have different predicting capabilities for a certain event. Notwithstanding, not much work is available for assessing the efficacy of semantic representation from the source-level. On the other hand, it is plausible to perceive that some concepts are noisy for detecting a specific event. Motivated by these two shortcomings, we propose a bi-level semantic representation analyzing method. Regarding source-level, our method learns weights of semantic representation attained from different multimedia archives. Meanwhile, it restrains the negative influence of noisy or irrelevant concepts in the overall concept-level. In addition, we particularly focus on efficient multimedia event detection with few positive examples, which is highly appreciated in the real-world scenario. We perform extensive experiments on the challenging TRECVID MED 2013 and 2014 datasets with encouraging results that validate the efficacy of our proposed approach.", "desc": "detect video events with multimedia analysis", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 51, "fields": {"turk_id": "Q6mpd5FYHQ", "abstract": "Many information-extraction and knowledge base construction systems are addressing the challenge of deriving knowledge from text. A key problem in constructing these knowledge bases from sources like the web is overcoming the erroneous and incomplete informa- tion found in millions of candidate extractions. To solve this problem, we turn to semantics \u2030\u0170\u00d3 using ontological constraints between candidate facts to eliminate errors. In this article, we represent the desired knowledge base as a knowledge graph and introduce the problem of knowledge graph identifica- tion, collectively resolving the entities, labels, and relations present in the knowledge graph. Knowledge graph identification requires reasoning jointly over millions of extractions simultane- ously, posing a scalability challenge to many approaches. We use probabilistic soft logic (PSL), a recently introduced statistical relational learning framework, to implement an efficient solution to knowledge graph identification and present state-of-the-art results for knowledge graph construction while performing an order of magnitude faster than competing methods.", "desc": "extract information for a knowledgebase when the candidate extractions have erronious and incomplete information", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 52, "fields": {"turk_id": "kzkU84mAfQ", "abstract": "Studying characters plays a vital role in computationally representing and interpreting narratives. Unlike previous work, which has focused on inferring character roles, we focus on the problem of modeling their relationships. Rather than assuming a fixed relationship for a character pair, we hypothesize that relationships temporally evolve with the progress of the narrative, and formulate the problem of relationship modeling as a structured prediction problem. We propose a semi-supervised framework to learn relationship sequences from fully as well as partially labeled data. We present a Markovian model capable of accumulating historical beliefs about the relationship and status changes. We  use a set of rich linguistic and semantically motivated features that incorporate world knowledge to investigate the textual content of narrative. We empirically demonstrate that such a framework outperforms competitive baselines.", "desc": "interperet narrative texts computationally", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 53, "fields": {"turk_id": "pz1FeHpIbO", "abstract": "While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difficult to interpret. For example it's not clear how they achieve {\\em compositionality}, building sentence meaning from the meanings of words and phrases. In this paper we describe four strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allow us to see well-known markedness asymmetries in negation. We then introduce three simple and straightforward methods for visualizing a unit's {\\em salience}, the amount it contributes to the final composed meaning: (1) gradient back-propagation, (2) the variance of a token from the average word node, (3) LSTM-style gates that measure information flow. We test our methods on sentiment using simple recurrent nets and LSTMs. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks , and also shed light on why LSTMs outperform simple recurrent nets,", "desc": "visualize compositionality in neural models for natural langauge processing", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 54, "fields": {"turk_id": "sq35EuQC1F", "abstract": "Provides an overview of a speech-to-text (STT) and keyword search (KWS) system architecture build primarily on the top of the Kaldi toolkit and expands on a few highlights. The system was developed as a part of the research efforts of the Radical team while participating in the IARPA Babel program. Our aim was to develop a general system pipeline which could be easily and rapidly deployed in any language, independently on the language script and phonological and linguistic features of the language.", "desc": "transcribe speech using a speech-to-text (STT) and keyword search (KWS) system architecture", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 55, "fields": {"turk_id": "ZND7bbmGBM", "abstract": "In predicate invention (PI), new predicates are introduced into a logical theory, usually by rewriting a group of closely-related rules to use a common invented predicate as a \u2030\u0170\u010esubroutine\u2030\u0170\u0165. PI is difficult, since a poorly-chosen invented predicate may lead to error cascades. Here we suggest a \u2030\u0170\u010esoft\u2030\u0170\u0165 version of predicate invention: instead of explicitly creating new predicates, we implicitly group closely-related rules by using structured sparsity to regularize their parameters together. We show that soft PI, unlike hard PI, consistently improves over previous strong baselines for structure-learning on two large-scale tasks.", "desc": "invent new predicates for a logical theory when a poorly-chosen predicate causes cascading errors", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 56, "fields": {"turk_id": "6OWup2PA6C", "abstract": "A standard pipeline for statistical relational learning involves two steps: one first constructs the knowledge base (KB) from text, and then performs the learning and reasoning tasks using probabilistic first-order logics. However, a key issue is that information extraction (IE) er- rors from text affect the quality of the KB, and propagate to the reasoning task. In this paper, we propose a statistical rela- tional learning model for joint information extraction and reasoning. More specifically, we incorporate context-based entity extraction with structure learning (SL) in a scalable probabilistic logic framework. We then propose a latent context invention (LCI) approach to improve the per- formance. In experiments, we show that our approach outperforms state-of-the-art baselines over three real-world Wikipedia datasets from multiple domains; that joint learning and inference for IE and SL significantly improve both tasks; that latent context invention further improves the results.", "desc": "extract information for a knowledgebase accurately", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 57, "fields": {"turk_id": "5QS4zYFEwQ", "abstract": "Coreference resolution is one of the first stages in deep language understanding and its importance has been well recognized in the natural language processing community. In this paper, we propose a generative, unsupervised ranking model for entity coreference resolution by introducing resolution mode variables. Our unsupervised system achieves 58.44% F1 score of the CoNLL metric on the English data from the CoNLL-2012 shared task (Pradhan et al., 2012), outperforming the Stanford deterministic system (Lee et al., 2013) by 3.01%.", "desc": "resolve coreferencing", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 58, "fields": {"turk_id": "etmYQfy2Fs", "abstract": "Understanding the purpose of why sensitive data is used could help improve privacy as well as enable new kinds of access control. In this paper, we introduce a new technique for inferring the purpose of sensitive data usage in the context of Android smartphone apps. We extract multiple kinds of features from decompiled code, focusing on app-specific features and text-based features. These features are then used to train a machine learning classifier. We have evaluated our approach in the context of two sensitive permissions, namely ACCESS_FINE_LOCATION and READ_CONTACT_LIST, and achieved an accuracy of about 85% and 94% respectively in inferring purposes. We have also found that text-based features alone are highly effective in inferring purposes.", "desc": "access accounts with improved privacy and enable new kinds of access", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 59, "fields": {"turk_id": "nkQda1DyQb", "abstract": "In this paper, we focus on automatically detecting events in unconstrained videos without the use of any visual training exemplars. In principle, zero-shot learning makes it possible to train an event detection model based on the assumption that events (e.g. \\emph{birthday party}) can be described by multiple mid-level semantic concepts (e.g. \"blowing candle\", \"birthday cake\"). Towards this goal, we first pre-train a bundle of concept classifiers using data from other sources. Then we evaluate the semantic correlation of each concept \\wrt the event of interest and pick up the relevant concept classifiers, which are applied on all test videos to get multiple prediction score vectors. While most existing systems combine the predictions of the concept classifiers with fixed weights, we propose to learn the optimal weights of the concept classifiers for each testing video by exploring a set of online available videos with free-form text descriptions of their content. To validate the effectiveness of the proposed approach, we have conducted extensive experiments on the latest TRECVID MEDTest 2014, MEDTest 2013 and CCV dataset. The experimental results confirm the superiority of the proposed approach.", "desc": "detect events in unconstrained videos automatically without the use of any visual training exemplars", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 60, "fields": {"turk_id": "sHW1wlH1LM", "abstract": "In particular for 'low resource' Keyword Search (KWS) and Speech-to-Text (STT) tasks, more untranscribed test data may be available than training data. Several approaches have been proposed to make this data useful during system development, even when initial systems have Word Error Rates (WER) above 70%. In this paper, we present a set of experiments on low-resource languages in telephony speech quality in Assamese, Bengali, Lao, Haitian, Zulu, and Tamil, demonstrating the impact that such techniques can have, in particular learning robust bottle-neck features on the test data. In the case of Tamil, when significantly more test data than training data is available, we integrated semi-supervised training and speaker adaptation on the test data, and achieved significant additional improvements in STT and KWS.", "desc": "transcribe speech when more untranscribed test data than training data is available", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 61, "fields": {"turk_id": "CC0tM8s8ty", "abstract": "In acoustic modeling, speaker adaptive training (SAT) has been a long-standing technique for the traditional Gaussian mixture models (GMMs). Acoustic models trained with SAT become independent of training speakers and generalize better to unseen testing speakers. This paper ports the idea of SAT to deep neural networks (DNNs), and proposes a framework to perform feature-space SAT for DNNs. Using i-vectors as speaker representations, our framework learns an adaptation neural network to derive speaker-normalized features. Speaker adaptive models are obtained by fine-tuning DNNs in such a feature space. This framework can be applied to various feature types and network structures, posing a very general SAT solution. In this paper, we fully investigate how to build SAT-DNN models effectively and efficiently. First, we study the optimal configurations of SAT-DNNs for large-scale acoustic modeling tasks. Then, after presenting detailed comparisons between SAT-DNNs and the existing DNN adaptation methods, we propose to combine SAT-DNNs and model-space DNN adaptation during decoding. Finally, to accelerate learning of SAT-DNNs, a simple yet effective strategy, frame skipping, is employed to reduce the size of training data. Our experiments show that compared with a strong DNN baseline, the SAT-DNN model achieves 13.5% and 17.5% relative improvement on word error rates (WERs), without and with model-space adaptation applied respectively. Data reduction based on frame skipping results in 2 \u011a\u0143 speed-up for SAT-DNN training, while causing negligible WER loss on the testing data.", "desc": "perform feature-space Speaker Adaptive Training for Deep Neural Networks", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 62, "fields": {"turk_id": "6jSu6zItXp", "abstract": "Building an intelligent agent that simulates human learning of math and science could potentially benefit both cognitive science, by contributing to the understanding of human learning, and artificial intelligence, by advancing the goal of creating human-level intelligence. However, constructing such a learning agent currently requires manual encoding of prior domain knowledge; in addition to being a poor model of human acquisition of prior knowledge, manual knowledge-encoding is both time-consuming and error-prone. Previous research has shown that one of the key factors that differentiates experts and novices is their different representations of knowledge. Experts view the world in terms of deep functional features, while novices view it in terms of shallow perceptual features. Moreover, since the performance of learning algorithms is sensitive to representation, the deep features are also important in achieving effective machine learning. In this paper, we present an efficient algorithm that acquires representation knowledge in the form of \u2030\u0170\u010edeep features\u2030\u0170\u0165, and demonstrate its effectiveness in the domain of algebra as well as synthetic domains. We integrate this algorithm into a machine-learning agent, SimStudent, which learns procedural knowledge by observing a tutor solve sample problems, and by getting feedback while actively solving problems on its own. We show that learning \u2030\u0170\u010edeep features\u2030\u0170\u0165 reduces the requirements for knowledge engineering. Moreover, we propose an approach that automatically discovers student models using the extended SimStudent. By fitting the discovered model to real student learning curve data, we show that it is a better student model than human-generated models, and demonstrate how the discovered model may be used to improve a tutoring system's instructional strategy.", "desc": "simulate human learning of math and science with an intelligent agent", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 63, "fields": {"turk_id": "iuA67ysj3n", "abstract": "This paper describes GenProg, an automated method for repairing defects in off-the-shelf, legacy programs without formal specifications, program annotations, or special coding practices. GenProg uses an extended form of genetic programming to evolve a program variant that retains required functionality but is not susceptible to a given defect, using existing test suites to encode both the defect and required functionality. Structural differencing algorithms and delta debugging reduce the difference between this variant and the original program to a minimal repair. We describe the algorithm and report experimental results of its success on 16 programs totaling 1.25 M lines of C code and 120K lines of module code, spanning eight classes of defects, in 357 seconds, on average. We analyze the generated repairs qualitatively and quantitatively to demonstrate that the process efficiently produces evolved programs that repair the defect, are not fragile input memorizations, and do not lead to serious degradation in functionality.", "desc": "repair defects in off-the-shelf programs without formal specifications, program annotations, or special coding practices", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 64, "fields": {"turk_id": "YAn1rzqnOh", "abstract": "Multimedia event detection (MED) and multimedia event recounting (MER) are fundamental tasks in managing large amounts of unconstrained web videos, and have attracted a lot of attention in recent years. Most existing systems perform MER as a post-processing step on top of the MED results. In order to leverage the mutual benefits of the two tasks, we propose a joint framework that simultaneously detects high-level events and localizes the indicative concepts of the events. Our premise is that a good recounting algorithm should not only explain the detection result, but should also be able to assist detection in the first place. Coupled in a joint optimization framework, recounting improves detection by pruning irrelevant noisy concepts while detection directs recounting to the most discriminative evidences. To better utilize the powerful and interpretable semantic video representation, we segment each video into several shots and exploit the rich temporal structures at shot level. The consequent computational challenge is carefully addressed through a significant improvement of the current ADMM algorithm, which, after eliminating all inner loops and equipping novel closed-form solutions for all intermediate steps, enables us to efficiently process extremely large video corpora. We test the proposed method on the large scale TRECVID MEDTest 2014 and MEDTest 2013 datasets, and obtain very promising results for both MED and MER.", "desc": "perform MER with MED simultaneously", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 65, "fields": {"turk_id": "ol5Wzanicu", "abstract": "In this chapter we describe the design, development and application of the Helix Metamorphic Shield (HMS). The HMS: (1) continuously shifts the program\u2030\u0170\u015es attack surface in both the spatial and temporal dimensions, and (2), reduces the program\u2030\u0170\u015es attack surface by applying novel evolutionary algorithms to automatically repair vulnerabilities. The symbiotic interplay between shifting and reducing the attack surface results in the automated evolution of new program variants whose quality improves over time.", "desc": "protect program from attack", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 66, "fields": {"turk_id": "IaBOLlTBj7", "abstract": "large-scale idea generation platforms often expose ideators to previous ideas. However, research suggests people generate better ideas if they see abstracted solution paths (e.g., descriptions of solution approaches generated through human sensemaking) rather than being inundated with all prior ideas. Automated and semi-automated methods can also offer interpretations of earlier ideas. To benefit from sensemaking in practice with limited resources, ideation platform developers need to weigh the cost-quality tradeoffs of different methods for surfacing solution paths. To explore this, we conducted an online study where 245 participants generated ideas for two problems in one of five conditions: 1) no stimuli, 2) exposure to all prior ideas, or solution paths extracted from prior ideas using 3) a fully automated workflow, 4) a hybrid human-machine approach, and 5) a fully manual approach. Contrary to expectations, human-generated paths did not improve ideation (as meas- ured by fluency and breadth of ideation) over simply showing all ideas. Machine-generated paths sometimes significantly improved fluency and breadth of ideation over no ideas (although at some cost to idea quality). These findings suggest that automated sensemaking can improve idea generation, but we need more research to understand the value of human sensemaking for crowd ideation.", "desc": "generate better ideas when ideators see abstracted solution paths", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 67, "fields": {"turk_id": "BzhW0GDGLu", "abstract": "State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks on different languages. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\\% accuracy for POS tagging and 91.21\\% F1 for NER.", "desc": "sequence labling without  large amounts of task-specific prior knowledge", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 68, "fields": {"turk_id": "AOR3A6bJR6", "abstract": "Twitter, Flickr, Instagram, and other public social media sites have inspired lots of analysis of public geotagged posts. In order to understand these posts, it is important to know where their authors live. Based on a study of 195 prolific Twitter users in the Pittsburgh area, and their ground truth home locations, we show that simple algorithms can find about 80% of people\u2030\u0170\u015es home addresses within 1 kilometer. We show why this is near the upper bound of feasibility, show that studying as few as 10 tweets can achieve almost the same results, and dis- cuss implications for future social media analyses.", "desc": "find the source location of social media data", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 69, "fields": {"turk_id": "ti30tF5nHL", "abstract": "One important challenge for probabilistic logics is reasoning with very large knowledge bases (KBs) of imperfect information, such as those produced by modern web-scale information extraction systems. One scalability problem shared by many probabilistic logics is that answering queries involves \u2030\u0170\u010egrounding\u2030\u0170\u0165 the query\u2030\u0170\u00d3i.e., mapping it to a propositional representation\u2030\u0170\u00d3and the size of a \u2030\u0170\u010egrounding\u2030\u0170\u0165 grows with database size. To address this bottleneck, we present a first-order probabilistic language called ProPPR in which approximate \u2030\u0170\u010elocal groundings\u2030\u0170\u0165 can be constructed in time independent of database size. Technically, ProPPR is an extension to stochastic logic programs that is biased towards short derivations; it is also closely related to an earlier relational learning algorithm called the path ranking algorithm. We show that the problem of constructing proofs for this logic is related to computation of personalized PageRank on a linearized version of the proof space, and based on this connection, we develop a provably-correct approximate grounding scheme, based on the PageRank\u2030\u0170\u0147Nibble algorithm. Building on this, we develop a fast and easily-parallelized weight-learning algorithm for ProPPR. In our experiments, we show that learning for ProPPR is orders of magnitude faster than learning for Markov logic networks; that allowing mutual recursion (joint learning) in KB inference leads to improvements in performance; and that ProPPR can learn weights for a mutually recursive program with hundreds of clauses defining scores of interrelated predicates over a KB containing one million entities.", "desc": "reason with very large knowledge bases (KBs) of imperfect information", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 70, "fields": {"turk_id": "MNOr7P2CdP", "abstract": "Friendsourcing consists of broadcasting questions and help requests to friends on social networking sites. Despite its potential value, friendsourcing requests often fall on deaf ears. One way to improve response rates and motivate friends to undertake more effortful tasks may be to offer extrinsic rewards, such as money or a gift, for responding to friendsourcing requests. However, past research suggests that these extrinsic rewards can have unintended consequences, including undermining intrinsic motivations and undercutting the relationship between people. To explore the effects of extrinsic reward on friends\u2030\u0170\u015e response rate and perceived relationship, we conducted an experiment on a new friendsourcing platform - Mobilyzr. Results indicate that large extrinsic rewards increase friends\u2030\u0170\u015e response rates without reducing the relationship strength between friends. Additionally, the extrinsic rewards allow requesters to explain away the failure of friendsourcing requests and thus preserve their perceptions of relationship ties with friends.", "desc": "improve friendsourcing, the idea that friends can ask eachother for help", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 71, "fields": {"turk_id": "QNFR41s1Ya", "abstract": "Peer assessment helps students reflect and exposes them to different ideas. It scales assessment and allows large online classes to use open-ended assignments. However, it requires students to spend significant time grading. How can we lower this grading burden while maintaining quality? This paper integrates peer and machine grading to preserve the robustness of peer assessment and lower grading burden. In the identify-verify pattern, a grading algorithm first predicts a student grade and estimates confidence, which is used to estimate the number of peer raters required. Peers then identify key features of the answer using a rubric. Finally, other peers verify whether these feature labels were accurately applied. This pattern adjusts the number of peers that evaluate an answer based on algorithmic confidence and peer agreement. We evaluated this pattern with 1370 students in a large, online design class. With only 54% of the student grading time, the identify-verify pattern yields 80-90% of the accuracy obtained by taking the median of three peer scores, and provides more detailed feedback. A second experiment found that verification dramatically improves accuracy with more raters, with a 20% gain over the peer-median with four raters. However, verification also leads to lower initial trust in the grading system. The identify-verify pattern provides an example of how peer work and machine learning can combine to improve the learning experience.", "desc": "reduce the burden of peer grading while maintaining quality", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 72, "fields": {"turk_id": "OzznPWO3B4", "abstract": "State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available. In this paper, we introduce two new neural architectures---one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers. Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora. Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers.", "desc": "recognize named entities without relying on hand-crafted features or domain-specific knowledge", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 73, "fields": {"turk_id": "0q3NPaGLtH", "abstract": "Programming languages often include specialized syntax for common datatypes (e.g. lists) and some also build in support for specific specialized datatypes (e.g. regular expressions), but user-defined types must use general-purpose syntax. Frustration with this causes developers to use strings, rather than structured data, with alarming frequency, leading to correctness, performance, security, and usability issues. Allowing library providers to modularly extend a language with new syntax could help address these issues. Unfortunately, prior mechanisms either limit expressiveness or are not safely composable: individually unambiguous extensions can still cause ambiguities when used together. We introduce type-specific languages (TSLs): logic associated with a type that determines how the bodies of generic literals, able to contain arbitrary syntax, are parsed and elaborated, hygienically. The TSL for a type is invoked only when a literal appears where a term of that type is expected, guaranteeing non-interference. We give evidence supporting the applicability of this approach and formally specify it with a bidirectionally typed elaboration semantics for the Wyvern programming language.", "desc": "Allow library providers to modularly extend a language with new syntax", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 74, "fields": {"turk_id": "876Z7xu4mK", "abstract": "Early detection and precise characterization of emerging topics in text streams can be highly useful in applications such as timely and targeted public health interventions and discovering evolving regional business trends. Many methods have been proposed for detecting emerging events in text streams using topic modeling. However, these methods have numerous shortcomings that make them unsuitable for rapid detection of locally emerging events on massive text streams. In this paper, we describe Semantic Scan (SS) that has been developed specifically to overcome these shortcomings in detecting new spatially compact events in text streams. Semantic Scan integrates novel contrastive topic modeling with online document assignment and principled likelihood ratio-based spatial scanning to identify emerging events with unexpected patterns of keywords hidden in text streams. This enables more timely and accurate detection and characterization of anomalous, spatially localized emerging events. Semantic Scan does not require manual intervention or labeled training data, and is robust to noise in real-world text data since it identifies anomalous text patterns that occur in a cluster of new documents rather than an anomaly in a single new document. We compare Semantic Scan to alternative state-of-the-art methods such as Topics over Time, Online LDA, and Labeled LDA on two real-world tasks: (i) a disease surveillance task monitoring free-text Emergency Department chief complaints in Allegheny County, and (ii) an emerging business trend detection task based on Yelp reviews. On both tasks, we find that Semantic Scan provides significantly better event detection and characterization accuracy than competing approaches, while providing up to an order of magnitude speedup.", "desc": "detect new spatially compact events in text streams", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 75, "fields": {"turk_id": "stnrQPSKUA", "abstract": "Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce unpredictability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.", "desc": "Combine deep neural networks with structured logic rules", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 76, "fields": {"turk_id": "z743brRxxo", "abstract": "The performance of automatic speech recognition (ASR) has improved tremendously due to the application of deep neu- ral networks (DNNs). Despite this progress, building a new ASR system remains a challenging task, requiring various resources, multiple training stages and significant expertise. This paper presents our Eesen framework which drastically simplifies the existing pipeline to build state-of-the-art ASR systems. Acoustic modeling in Eesen involves learning a single recurrent neural network (RNN) predicting context- independent targets (phonemes or characters). To remove the need for pre-generated frame labels, we adopt the connectionist temporal classification (CTC) objective function to infer the alignments between speech and label sequences. A distinctive feature of Eesen is a generalized decoding approach based on weighted finite-state transducers (WFSTs), which enables the efficient incorporation of lexicons and language models into CTC decoding. Experiments show that compared with the standard hybrid DNN systems, Eesen achieves comparable word error rates (WERs), while at the same time speeding up decoding significantly.", "desc": "transcribe speech", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 77, "fields": {"turk_id": "8e1uALXkud", "abstract": "While there has been much research on automatically constructing structured Knowledge Bases (KBs), most of it has focused on generating facts to populate a KB. However, a useful KB must go beyond facts. For example, glosses (short natural language definitions) have been found to be very useful in tasks such as Word Sense Disambiguation. However, the important problem of Automatic Gloss Finding, i.e., assigning glosses to entities in an initially gloss-free KB, is relatively unexplored. We address that gap in this paper. In particular, we propose GLOFIN, a hierarchical semi-supervised learning algorithm for this problem which makes effective use of limited amounts of supervision and available ontological constraints. To the best of our knowledge, GLOFIN is the first system for this task. Through extensive experiments on real-world datasets, we demonstrate GLOFIN's effectiveness. It is encouraging to see that GLOFIN outperforms other state-of-the-art SSL algorithms, especially in low supervision settings. We also demonstrate GLOFIN's robustness to noise through experiments on a wide variety of KBs, ranging from user contributed (e.g., Freebase) to automatically constructed (e.g., NELL). To facilitate further research in this area, we have made the datasets and code used in this paper publicly available.", "desc": "connect glosses (short natural language definitions) with facts in a knowledge base", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 78, "fields": {"turk_id": "IdhNPy4nJk", "abstract": "In this paper, we propose a novel approach for Word Sense Disambiguation (WSD) of verbs that can be applied directly in the event mention detection task to classify event types. By using the PropStore, a database of relations between words, our approach disambiguates senses of verbs by utilizing the information of verbs that appear in similar syntactic contexts. Importantly, the resource our approach requires is only a word sense dictionary, without any annotated sentences or structures and relations between different senses (as in WordNet). Our approach can be extended to disambiguate senses of words for parts of speech besides verbs.", "desc": "disambiguate senses of verbs", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 79, "fields": {"turk_id": "7gmc9in5zb", "abstract": "Understanding the social roles played by contributors to online communities can facilitate the process of task routing. In this work, we develop new techniques to find roles in Wikipedia based on editors\u2030\u0170\u015e low-level edit types and investigate how work contributed by people from different roles affect the article quality. To do this, we first built machinelearning models to automatically identify the edit categories associated with edits. We then applied a graphical model analogous to Latent Dirichlet Allocation to uncover the latent roles in editors\u2030\u0170\u015e edit histories. Applying this technique revealed eight different roles editors play. Finally, we validated how our identified roles collaborate to improve the quality of articles. The results demonstrate that editors carrying on different roles contribute differently in terms of edit categories and articles in different quality stages need different types of editors. Implications for editor role identification and the validation of role contribution are discussed.", "desc": "Understand the social roles played by contributors to online communities", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 80, "fields": {"turk_id": "ZC1xB4ijlb", "abstract": "Typestate reflects how the legal operations on imperative objects can change at runtime as their internal state changes. A typestate checker can statically ensure, for instance, that an object method is only called when the object is in a state for which the operation is well defined. Prior work has shown how modular typestate checking can be achieved thanks to access permissions and state guarantees. However, typestate was not treated as a primitive language concept: typestate checkers are an additional verification layer on top of an existing language. In contrast, a typestate-oriented programming (TSOP) language directly supports expressing typestates. For example, in the Plaid programming language, the typestate of an object directly corresponds to its class, and that class can change dynamically. Plaid objects have not only typestate-dependent interfaces but also typestate-dependent behaviors and runtime representations. This article lays foundations for TSOP by formalizing a nominal object-oriented language with mutable state that integrates typestate change and typestate checking as primitive concepts. We first describe a statically typed language\u2030\u0170\u00d3Featherweight Typestate (FT)\u2030\u0170\u00d3where the types of object references are augmented with access permissions and state guarantees. We describe a novel flow-sensitive permission-based type system for FT. Because static typestate checking is still too rigid for some applications, we then extend this language into a gradually typed language\u2030\u0170\u00d3Gradual Featherweight Typestate (GFT). This language extends the notion of gradual typing to account for typestate: gradual typestate checking seamlessly combines static and dynamic checking by automatically inserting runtime checks into programs. The gradual type system of GFT allows programmers to write dynamically safe code even when the static type checker can only partly verify it.", "desc": "develop typestate-oriented programming", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 81, "fields": {"turk_id": "eHhLNYRnjU", "abstract": "Vast quantities of videos are now being captured at astonishing rates, but the majority of these are not labelled.  To cope with such data, we consider the task of content-based activity recognition in videos without any manually labelled examples, also known as zero-shot video recognition. To achieve this, videos are represented in  terms of detected visual concepts, which are then scored as relevant or irrelevant according to their similarity with a given textual query.  In this paper, we propose a more robust approach for scoring concepts in order to alleviate many of the brittleness and low precision problems of previous work. Not only do we jointly consider semantic relatedness, visual reliability, and discriminative power. To handle noise and non-linearities in the ranking scores of the selected concepts, we propose a novel pairwise order matrix approach for score aggregation. Extensive experiments on the large-scale TRECVID Multimedia Event Detection data show the superiority of our approach.", "desc": "recognize activity in videos using content-based recognition without manually labelled examples", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 82, "fields": {"turk_id": "60abWfXMXp", "abstract": "Automatic real-time captioning provides immediate and on demand access to spoken content in lectures or talks, and is a crucial accommodation for deaf and hard of hearing (DHH) people. However, in the presence of specialized content, like in technical talks, automatic speech recognition (ASR) still makes mistakes which may render the output incomprehensible. In this paper, we introduce a new approach, which allows audience or crowd workers, to quickly correct errors that they spot in ASR output. Prior approaches required the crowd worker to manually \u2030\u0170\u010eedit\u2030\u0170\u0165 the ASR hypothesis by selecting and replacing the text, which is not suitable for real-time scenarios. Our approach is faster and allows the worker to simply type corrections for misrecognized words as soon as he or she spots them. The system then finds the most likely position for the correction in the ASR output using keyword search (KWS) and stitches the word into the ASR output. Our work demonstrates the potential of computation to incorporate human input quickly enough to be usable in real-time scenarios, and may be a better method for providing this vital accommodation to DHH people.", "desc": "speed up manual correction of captions", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 83, "fields": {"turk_id": "ehOfqxRrFL", "abstract": "We present an approach for the detection of coordinate-term relationships between entities from the software domain, that refer to Java classes. Usually, relations are found by examining corpus statistics associated with text entities. In some technical domains, however, we have access to additional information about the real-world objects named by the entities, suggesting that coupling information about the \"grounded\" entities with corpus statistics might lead to improved methods for relation discovery. To this end, we develop a similarity measure for Java classes using distributional information about how they are used in software, which we combine with corpus statistics on the distribution of contexts in which the classes appear in text. Using our approach, cross-validation accuracy on this dataset can be improved dramatically, from around 60% to 88%. Human labeling results show that our classifier has an F1 score of 86% over the top 1000 predicted pairs.", "desc": "relate software entities (java classes) to real world objects", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 84, "fields": {"turk_id": "1ewbCznf4F", "abstract": "History event related knowledge is precious and imagery is a powerful medium that records diverse information about the event. In this paper, we propose to automatically construct an image profile given a one sentence description of the historic event which contains where, when, who and what elements. Such a simple input requirement makes our solution easy to scale up and support a wide range of culture preservation and curation related applications ranging from wikipedia enrichment to history education. However, history relevant information on the web is available as \"wild and dirty\" data, which is quite different from clean, manually curated and structured information sources. There are two major challenges to build our proposed image profiles: 1) unconstrained image genre diversity. We categorize images into genres of documents/maps, paintings or photos. Image genre classification involves a full-spectrum of features from low-level color to high-level semantic concepts. 2) image content diversity. It can include faces, objects and scenes. Furthermore, even within the same event, the views and subjects of images are diverse and correspond to different facets of the event. To solve this challenge, we group images at two levels of granularity: iconic image grouping and facet image grouping. These require different types of features and analysis from near exact matching to soft semantic similarity. We develop a full-range feature analysis module which is composed of several levels, each suitable for different types of image analysis tasks. The wide range of features are based on both classical hand-crafted features and different layers of a convolutional neural network. We compare and study the performance of the different levels in the full-range features and show their effectiveness on handling such a wild, unconstrained dataset.", "desc": "manage unconstrained image genre diversity", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 85, "fields": {"turk_id": "PWdaJSkSJz", "abstract": "Microblogs such as Twitter, Facebook, and Sina Weibo (China's equivalent of Twitter), are a remarkable linguistic resource. In contrast to content from edited genres such as newswire, microblogs contain discussions of virtually every topic by numerous individuals in different languages and dialects and in different styles. In this work, we show that some microblog users post \u2030\u0170\u010eself-translated\u2030\u0170\u0165 messages targeting audiences who speak different languages, either by writing the same message in multiple languages or by retweeting translations of their original posts in a second language. We introduce a method for finding and extracting this naturally occurring parallel data. Identifying the parallel content requires solving an alignment problem, and we give an optimally efficient dynamic programming algorithm for this. Using our method, we extract nearly 3M Chinese\u2030\u0170\u0147English parallel segments from Sina Weibo using a targeted crawl of Weibo users who post in multiple languages. Additionally, from a random sample of Twitter, we are obtain substantial amounts of parallel data in multiple language pairs. Evaluation is performed by assessing the accuracy of our extraction approach relative to a manual annotation as well as in terms of utility as training data for a Chinese\u2030\u0170\u0147English machine translation system. Relative to traditional parallel data resources, the automatically extracted parallel data yields substantial translation quality improvements in translating microblog text and modest improvements in translating edited news content.", "desc": "solve an alignment problem to extract naturally occurring parallel data in social media user's self-translations", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 86, "fields": {"turk_id": "M428ps2Ieq", "abstract": "Words are polysemous. However, most approaches to representation learning for lexical semantics assign a single vector to every surface word type. Meanwhile, lexical ontologies such as WordNet provide a source of complementary knowledge to distributional information, including a word sense inventory. In this paper we propose two novel and general approaches for generating sense-specific word embeddings that are grounded in an ontology. The first applies graph smoothing as a postprocessing step to tease the vectors of different senses apart, and is applicable to any vector space model. The second adapts predictive maximum likelihood models that learn word embeddings with latent variables representing senses grounded in an specified ontology. Empirical results on lexical semantic tasks show that our approaches effectively captures information from both the ontology and distributional statistics. Moreover, in most cases our sense-specific models outperform other models we compare against.", "desc": "differentiate between the multiple meanings or senses of a word", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 87, "fields": {"turk_id": "MUt1MbjBFa", "abstract": "The pervasiveness of mobile technologies today have facilitated the creation of massive crowdsourced and geotagged data from individual users in real time and at different locations in the city. Such ubiquitous user-generated data allow us to infer various patterns of human behavior, which help us understand the interactions between humans and cities. In this study, we focus on understanding users economic behavior in the city by examining the economic value from crowdsourced and geotaggged data. Specifically, we extract multiple traffic and human mobility features from publicly available data sources using NLP and geo-mapping techniques, and examine the effects of both static and dynamic features on economic outcome of local businesses. Our study is instantiated on a unique dataset of restaurant bookings from OpenTable for 3,187 restaurants in New York City from November 2013 to March 2014. Our results suggest that foot traffic can increase local popularity and business performance, while mobility and traffic from automobiles may hurt local businesses, especially the well-established chains and high-end restaurants. We also find that on average one more street closure nearby leads to a 4.7% decrease in the probability of a restaurant being fully booked during the dinner peak. Our study demonstrates the potential of how to best make use of the large volumes and diverse sources of crowdsourced and geotagged user-generated data to create matrices to predict local economic demand in a manner that is fast, cheap, accurate, and meaningful.", "desc": "use geotagged user data to create matrices to predict local economic demand", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 88, "fields": {"turk_id": "fhE2nxUl6x", "abstract": "Gaze-based interaction has several benefits: naturalism, remote controllability, and easy accessibility. However, it has been mostly used for screen-based interaction with static information. In this paper, we propose a concept of gaze-based interaction that augments the physical world with social information. We demonstrate this interaction in a shopping scenario. In-store shopping is a setting where social information can augment the physical environment to better support a user's purchase decision. Based on the user's ocular point, we project the following information on the product and its surrounding surface: collective in-store gazes and purchase data, product comparison information, animation expressing ingredient of product, and online social comments. This paper presents the design of the system, the results and discussion of an informal user study, and future work.", "desc": "augment the physical world with social information", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 89, "fields": {"turk_id": "er2gHc4u15", "abstract": "Modern smartphone platforms offer a multitude of useful features to their users but at the same time they are highly privacy affecting. However, smartphone platforms are not effective in properly communicating privacy risks to their users. Furthermore, common privacy risk communication approaches in smartphone app ecosystems do not consider the actual data-access behavior of individual apps in their risk assessments. Beyond privacy risks such as the leakage of single information (first-order privacy risk), we argue that privacy risk assessments and risk communication should also consider threats to user privacy coming from user-profiling and data-mining capabilities based on the long-term data-access behavior of apps (second-order privacy risk). In this paper, we introduce Styx, a novel privacy risk communication system for Android that provides users with privacy risk information based on the second-order privacy risk perspective. We discuss results from an experimental evaluation of Styx regarding its effectiveness in risk communication and its effects on user perceptions such as privacy concerns and the trustworthiness of a smartphone. Our results suggest that privacy risk information provided by Styx improves the comprehensibility of privacy risk information and helps the users in comparing different apps regarding their privacy properties. The results further suggest that an improved privacy risk communication on smartphones can increase trust towards a smartphone and reduce privacy concern.", "desc": "evaluate effect of the platform on user perceptions such as privacy concerns and the trustworthiness of a smartphone", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 90, "fields": {"turk_id": "s32EU2Ddyb", "abstract": "We propose two well-motivated ranking-based methods to enhance the performance of current state-of-the-art human activity recognition systems. First, as an improvement over the classic power normalization method, we propose a parameter-free ranking technique called rank normalization (RaN). RaN normalizes each dimension of the video features to address the sparse and bursty distribution problems of Fisher Vectors and VLAD. Second, inspired by curriculum learning, we introduce a training-free re-ranking technique called multi-class iterative re-ranking (MIR). MIR captures relationships among action classes by separating easy and typical videos from difficult ones and re-ranking the prediction scores of classifiers accordingly. We demonstrate that our methods significantly improve the performance of state-of-the-art motion features on six real-world datasets.", "desc": "enhance the performance of human activity recognition systems", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 91, "fields": {"turk_id": "xfAn6Bzf0y", "abstract": "Multimedia event detection has been one of the major endeavors in video event analysis. A variety of approaches have been proposed recently to tackle this problem. Among others, using semantic representation has been accredited for its promising performance and desirable ability for human-understandable reasoning. To generate semantic representation, we usually utilize several external image/video archives and apply the concept detectors trained on them to the event videos. Due to the intrinsic difference of these archives, the resulted representation is presumable to have different predicting capabilities for a certain event. Notwithstanding, not much work is available for assessing the efficacy of semantic representation from the source-level. On the other hand, it is plausible to perceive that some concepts are noisy for detecting a specific event. Motivated by these two shortcomings, we propose a bi-level semantic representation analyzing method. Regarding source-level, our method learns weights of semantic representation attained from different multimedia archives. Meanwhile, it restrains the negative influence of noisy or irrelevant concepts in the overall concept-level. In addition, we particularly focus on efficient multimedia event detection with few positive examples, which is highly appreciated in the real-world scenario. We perform extensive experiments on the challenging TRECVID MED 2013 and 2014 datasets with encouraging results that validate the efficacy of our proposed approach.", "desc": "balance different predicting capabilities for each source", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 92, "fields": {"turk_id": "kzkU84mAfQ", "abstract": "Studying characters plays a vital role in computationally representing and interpreting narratives. Unlike previous work, which has focused on inferring character roles, we focus on the problem of modeling their relationships. Rather than assuming a fixed relationship for a character pair, we hypothesize that relationships temporally evolve with the progress of the narrative, and formulate the problem of relationship modeling as a structured prediction problem. We propose a semi-supervised framework to learn relationship sequences from fully as well as partially labeled data. We present a Markovian model capable of accumulating historical beliefs about the relationship and status changes. We  use a set of rich linguistic and semantically motivated features that incorporate world knowledge to investigate the textual content of narrative. We empirically demonstrate that such a framework outperforms competitive baselines.", "desc": "model characters' relationships in a narrative text", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 93, "fields": {"turk_id": "ZND7bbmGBM", "abstract": "In predicate invention (PI), new predicates are introduced into a logical theory, usually by rewriting a group of closely-related rules to use a common invented predicate as a \u2030\u0170\u010esubroutine\u2030\u0170\u0165. PI is difficult, since a poorly-chosen invented predicate may lead to error cascades. Here we suggest a \u2030\u0170\u010esoft\u2030\u0170\u0165 version of predicate invention: instead of explicitly creating new predicates, we implicitly group closely-related rules by using structured sparsity to regularize their parameters together. We show that soft PI, unlike hard PI, consistently improves over previous strong baselines for structure-learning on two large-scale tasks.", "desc": "implicitly group closely-related rules instead of explicitly creating a new predicate", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 94, "fields": {"turk_id": "etmYQfy2Fs", "abstract": "Understanding the purpose of why sensitive data is used could help improve privacy as well as enable new kinds of access control. In this paper, we introduce a new technique for inferring the purpose of sensitive data usage in the context of Android smartphone apps. We extract multiple kinds of features from decompiled code, focusing on app-specific features and text-based features. These features are then used to train a machine learning classifier. We have evaluated our approach in the context of two sensitive permissions, namely ACCESS_FINE_LOCATION and READ_CONTACT_LIST, and achieved an accuracy of about 85% and 94% respectively in inferring purposes. We have also found that text-based features alone are highly effective in inferring purposes.", "desc": "infer the purpose of why sensitive data is used", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 95, "fields": {"turk_id": "nkQda1DyQb", "abstract": "In this paper, we focus on automatically detecting events in unconstrained videos without the use of any visual training exemplars. In principle, zero-shot learning makes it possible to train an event detection model based on the assumption that events (e.g. \\emph{birthday party}) can be described by multiple mid-level semantic concepts (e.g. \"blowing candle\", \"birthday cake\"). Towards this goal, we first pre-train a bundle of concept classifiers using data from other sources. Then we evaluate the semantic correlation of each concept \\wrt the event of interest and pick up the relevant concept classifiers, which are applied on all test videos to get multiple prediction score vectors. While most existing systems combine the predictions of the concept classifiers with fixed weights, we propose to learn the optimal weights of the concept classifiers for each testing video by exploring a set of online available videos with free-form text descriptions of their content. To validate the effectiveness of the proposed approach, we have conducted extensive experiments on the latest TRECVID MEDTest 2014, MEDTest 2013 and CCV dataset. The experimental results confirm the superiority of the proposed approach.", "desc": "learn the optimal weights of the concept classifiers for each testing video from free-form text descriptions", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 96, "fields": {"turk_id": "sHW1wlH1LM", "abstract": "In particular for 'low resource' Keyword Search (KWS) and Speech-to-Text (STT) tasks, more untranscribed test data may be available than training data. Several approaches have been proposed to make this data useful during system development, even when initial systems have Word Error Rates (WER) above 70%. In this paper, we present a set of experiments on low-resource languages in telephony speech quality in Assamese, Bengali, Lao, Haitian, Zulu, and Tamil, demonstrating the impact that such techniques can have, in particular learning robust bottle-neck features on the test data. In the case of Tamil, when significantly more test data than training data is available, we integrated semi-supervised training and speaker adaptation on the test data, and achieved significant additional improvements in STT and KWS.", "desc": "make untranscribed test data useful during system development when informaiton has high error rates", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 97, "fields": {"turk_id": "6jSu6zItXp", "abstract": "Building an intelligent agent that simulates human learning of math and science could potentially benefit both cognitive science, by contributing to the understanding of human learning, and artificial intelligence, by advancing the goal of creating human-level intelligence. However, constructing such a learning agent currently requires manual encoding of prior domain knowledge; in addition to being a poor model of human acquisition of prior knowledge, manual knowledge-encoding is both time-consuming and error-prone. Previous research has shown that one of the key factors that differentiates experts and novices is their different representations of knowledge. Experts view the world in terms of deep functional features, while novices view it in terms of shallow perceptual features. Moreover, since the performance of learning algorithms is sensitive to representation, the deep features are also important in achieving effective machine learning. In this paper, we present an efficient algorithm that acquires representation knowledge in the form of \u2030\u0170\u010edeep features\u2030\u0170\u0165, and demonstrate its effectiveness in the domain of algebra as well as synthetic domains. We integrate this algorithm into a machine-learning agent, SimStudent, which learns procedural knowledge by observing a tutor solve sample problems, and by getting feedback while actively solving problems on its own. We show that learning \u2030\u0170\u010edeep features\u2030\u0170\u0165 reduces the requirements for knowledge engineering. Moreover, we propose an approach that automatically discovers student models using the extended SimStudent. By fitting the discovered model to real student learning curve data, we show that it is a better student model than human-generated models, and demonstrate how the discovered model may be used to improve a tutoring system's instructional strategy.", "desc": "avoid manual encoding of prior domain knowledge", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 98, "fields": {"turk_id": "iuA67ysj3n", "abstract": "This paper describes GenProg, an automated method for repairing defects in off-the-shelf, legacy programs without formal specifications, program annotations, or special coding practices. GenProg uses an extended form of genetic programming to evolve a program variant that retains required functionality but is not susceptible to a given defect, using existing test suites to encode both the defect and required functionality. Structural differencing algorithms and delta debugging reduce the difference between this variant and the original program to a minimal repair. We describe the algorithm and report experimental results of its success on 16 programs totaling 1.25 M lines of C code and 120K lines of module code, spanning eight classes of defects, in 357 seconds, on average. We analyze the generated repairs qualitatively and quantitatively to demonstrate that the process efficiently produces evolved programs that repair the defect, are not fragile input memorizations, and do not lead to serious degradation in functionality.", "desc": "minimize repair section", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 99, "fields": {"turk_id": "YAn1rzqnOh", "abstract": "Multimedia event detection (MED) and multimedia event recounting (MER) are fundamental tasks in managing large amounts of unconstrained web videos, and have attracted a lot of attention in recent years. Most existing systems perform MER as a post-processing step on top of the MED results. In order to leverage the mutual benefits of the two tasks, we propose a joint framework that simultaneously detects high-level events and localizes the indicative concepts of the events. Our premise is that a good recounting algorithm should not only explain the detection result, but should also be able to assist detection in the first place. Coupled in a joint optimization framework, recounting improves detection by pruning irrelevant noisy concepts while detection directs recounting to the most discriminative evidences. To better utilize the powerful and interpretable semantic video representation, we segment each video into several shots and exploit the rich temporal structures at shot level. The consequent computational challenge is carefully addressed through a significant improvement of the current ADMM algorithm, which, after eliminating all inner loops and equipping novel closed-form solutions for all intermediate steps, enables us to efficiently process extremely large video corpora. We test the proposed method on the large scale TRECVID MEDTest 2014 and MEDTest 2013 datasets, and obtain very promising results for both MED and MER.", "desc": "detect high-level events and localize the indicative concepts of the events", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 100, "fields": {"turk_id": "IaBOLlTBj7", "abstract": "large-scale idea generation platforms often expose ideators to previous ideas. However, research suggests people generate better ideas if they see abstracted solution paths (e.g., descriptions of solution approaches generated through human sensemaking) rather than being inundated with all prior ideas. Automated and semi-automated methods can also offer interpretations of earlier ideas. To benefit from sensemaking in practice with limited resources, ideation platform developers need to weigh the cost-quality tradeoffs of different methods for surfacing solution paths. To explore this, we conducted an online study where 245 participants generated ideas for two problems in one of five conditions: 1) no stimuli, 2) exposure to all prior ideas, or solution paths extracted from prior ideas using 3) a fully automated workflow, 4) a hybrid human-machine approach, and 5) a fully manual approach. Contrary to expectations, human-generated paths did not improve ideation (as meas- ured by fluency and breadth of ideation) over simply showing all ideas. Machine-generated paths sometimes significantly improved fluency and breadth of ideation over no ideas (although at some cost to idea quality). These findings suggest that automated sensemaking can improve idea generation, but we need more research to understand the value of human sensemaking for crowd ideation.", "desc": "generate better ideas when ideators see abstracted solution paths rather than being inundated with all prior ideas", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 101, "fields": {"turk_id": "ti30tF5nHL", "abstract": "One important challenge for probabilistic logics is reasoning with very large knowledge bases (KBs) of imperfect information, such as those produced by modern web-scale information extraction systems. One scalability problem shared by many probabilistic logics is that answering queries involves \u2030\u0170\u010egrounding\u2030\u0170\u0165 the query\u2030\u0170\u00d3i.e., mapping it to a propositional representation\u2030\u0170\u00d3and the size of a \u2030\u0170\u010egrounding\u2030\u0170\u0165 grows with database size. To address this bottleneck, we present a first-order probabilistic language called ProPPR in which approximate \u2030\u0170\u010elocal groundings\u2030\u0170\u0165 can be constructed in time independent of database size. Technically, ProPPR is an extension to stochastic logic programs that is biased towards short derivations; it is also closely related to an earlier relational learning algorithm called the path ranking algorithm. We show that the problem of constructing proofs for this logic is related to computation of personalized PageRank on a linearized version of the proof space, and based on this connection, we develop a provably-correct approximate grounding scheme, based on the PageRank\u2030\u0170\u0147Nibble algorithm. Building on this, we develop a fast and easily-parallelized weight-learning algorithm for ProPPR. In our experiments, we show that learning for ProPPR is orders of magnitude faster than learning for Markov logic networks; that allowing mutual recursion (joint learning) in KB inference leads to improvements in performance; and that ProPPR can learn weights for a mutually recursive program with hundreds of clauses defining scores of interrelated predicates over a KB containing one million entities.", "desc": "map the querie of a knowledge base to a propositional representation", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 102, "fields": {"turk_id": "z743brRxxo", "abstract": "The performance of automatic speech recognition (ASR) has improved tremendously due to the application of deep neu- ral networks (DNNs). Despite this progress, building a new ASR system remains a challenging task, requiring various resources, multiple training stages and significant expertise. This paper presents our Eesen framework which drastically simplifies the existing pipeline to build state-of-the-art ASR systems. Acoustic modeling in Eesen involves learning a single recurrent neural network (RNN) predicting context- independent targets (phonemes or characters). To remove the need for pre-generated frame labels, we adopt the connectionist temporal classification (CTC) objective function to infer the alignments between speech and label sequences. A distinctive feature of Eesen is a generalized decoding approach based on weighted finite-state transducers (WFSTs), which enables the efficient incorporation of lexicons and language models into CTC decoding. Experiments show that compared with the standard hybrid DNN systems, Eesen achieves comparable word error rates (WERs), while at the same time speeding up decoding significantly.", "desc": "remove the need for pre-generated frame labels", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 103, "fields": {"turk_id": "60abWfXMXp", "abstract": "Automatic real-time captioning provides immediate and on demand access to spoken content in lectures or talks, and is a crucial accommodation for deaf and hard of hearing (DHH) people. However, in the presence of specialized content, like in technical talks, automatic speech recognition (ASR) still makes mistakes which may render the output incomprehensible. In this paper, we introduce a new approach, which allows audience or crowd workers, to quickly correct errors that they spot in ASR output. Prior approaches required the crowd worker to manually \u2030\u0170\u010eedit\u2030\u0170\u0165 the ASR hypothesis by selecting and replacing the text, which is not suitable for real-time scenarios. Our approach is faster and allows the worker to simply type corrections for misrecognized words as soon as he or she spots them. The system then finds the most likely position for the correction in the ASR output using keyword search (KWS) and stitches the word into the ASR output. Our work demonstrates the potential of computation to incorporate human input quickly enough to be usable in real-time scenarios, and may be a better method for providing this vital accommodation to DHH people.", "desc": "integrate corrections to software", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 104, "fields": {"turk_id": "1ewbCznf4F", "abstract": "History event related knowledge is precious and imagery is a powerful medium that records diverse information about the event. In this paper, we propose to automatically construct an image profile given a one sentence description of the historic event which contains where, when, who and what elements. Such a simple input requirement makes our solution easy to scale up and support a wide range of culture preservation and curation related applications ranging from wikipedia enrichment to history education. However, history relevant information on the web is available as \"wild and dirty\" data, which is quite different from clean, manually curated and structured information sources. There are two major challenges to build our proposed image profiles: 1) unconstrained image genre diversity. We categorize images into genres of documents/maps, paintings or photos. Image genre classification involves a full-spectrum of features from low-level color to high-level semantic concepts. 2) image content diversity. It can include faces, objects and scenes. Furthermore, even within the same event, the views and subjects of images are diverse and correspond to different facets of the event. To solve this challenge, we group images at two levels of granularity: iconic image grouping and facet image grouping. These require different types of features and analysis from near exact matching to soft semantic similarity. We develop a full-range feature analysis module which is composed of several levels, each suitable for different types of image analysis tasks. The wide range of features are based on both classical hand-crafted features and different layers of a convolutional neural network. We compare and study the performance of the different levels in the full-range features and show their effectiveness on handling such a wild, unconstrained dataset.", "desc": "manage image content diversity", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 105, "fields": {"turk_id": "s32EU2Ddyb", "abstract": "We propose two well-motivated ranking-based methods to enhance the performance of current state-of-the-art human activity recognition systems. First, as an improvement over the classic power normalization method, we propose a parameter-free ranking technique called rank normalization (RaN). RaN normalizes each dimension of the video features to address the sparse and bursty distribution problems of Fisher Vectors and VLAD. Second, inspired by curriculum learning, we introduce a training-free re-ranking technique called multi-class iterative re-ranking (MIR). MIR captures relationships among action classes by separating easy and typical videos from difficult ones and re-ranking the prediction scores of classifiers accordingly. We demonstrate that our methods significantly improve the performance of state-of-the-art motion features on six real-world datasets.", "desc": "separating easy and typical videos from difficult ones", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 106, "fields": {"turk_id": "xfAn6Bzf0y", "abstract": "Multimedia event detection has been one of the major endeavors in video event analysis. A variety of approaches have been proposed recently to tackle this problem. Among others, using semantic representation has been accredited for its promising performance and desirable ability for human-understandable reasoning. To generate semantic representation, we usually utilize several external image/video archives and apply the concept detectors trained on them to the event videos. Due to the intrinsic difference of these archives, the resulted representation is presumable to have different predicting capabilities for a certain event. Notwithstanding, not much work is available for assessing the efficacy of semantic representation from the source-level. On the other hand, it is plausible to perceive that some concepts are noisy for detecting a specific event. Motivated by these two shortcomings, we propose a bi-level semantic representation analyzing method. Regarding source-level, our method learns weights of semantic representation attained from different multimedia archives. Meanwhile, it restrains the negative influence of noisy or irrelevant concepts in the overall concept-level. In addition, we particularly focus on efficient multimedia event detection with few positive examples, which is highly appreciated in the real-world scenario. We perform extensive experiments on the challenging TRECVID MED 2013 and 2014 datasets with encouraging results that validate the efficacy of our proposed approach.", "desc": "eliminate noise for detecting a specific event", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 107, "fields": {"turk_id": "kzkU84mAfQ", "abstract": "Studying characters plays a vital role in computationally representing and interpreting narratives. Unlike previous work, which has focused on inferring character roles, we focus on the problem of modeling their relationships. Rather than assuming a fixed relationship for a character pair, we hypothesize that relationships temporally evolve with the progress of the narrative, and formulate the problem of relationship modeling as a structured prediction problem. We propose a semi-supervised framework to learn relationship sequences from fully as well as partially labeled data. We present a Markovian model capable of accumulating historical beliefs about the relationship and status changes. We  use a set of rich linguistic and semantically motivated features that incorporate world knowledge to investigate the textual content of narrative. We empirically demonstrate that such a framework outperforms competitive baselines.", "desc": "understand temporal relationships between characters in a narrative", "user": null, "trial": null, "type": "ok", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 108, "fields": {"turk_id": "Du816l6Nu9", "abstract": "A fundamental problem in editing shapes is the recognition of partial shapes in a drawing to which changes are to be made. In this paper the possibility of using shape rules as a mechanism for effecting such changes is explored. Shape rules represent spatial relationships between two shapes _ and _ with the interpretation that any instance of _ in a shape can be replaced by a similar instance of _. A shape generation system implemented in PROLOG is described.", "desc": "recognize partial shapes in a drawing to which changes are to be made using shape rules", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 109, "fields": {"turk_id": "E9ZLIPI9Xp", "abstract": "A shape grammar, which can exemplify and visually explain the style of Taiwanese traditional vernacular dwellings, is presented in the form of rules of composition that are derived from considerations of traditional processes of design and construction, and from cultural influences.", "desc": "exemplify and visually explain the style of Taiwanese traditional vernacular dwellings", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 110, "fields": {"turk_id": "EE7EpRhmkZ", "abstract": "As one emerging plasmonic material, graphene can support surface plasmons at infrared and terahertz frequencies with unprecedented properties due to the strong interactions between graphene and low-frequency photons. Since graphene surface plasmons exist in the infrared and terahertz regime, they can be thermally pumped (excited) by the infrared evanescent waves emitted from an object. Here we show that thermal graphene plasmons can be efficiently excited and have monochromatic and tunable spectra, thus paving a way to harness thermal energy for graphene plasmonic devices. We further demonstrate that thermal information communication via graphene surface plasmons can be potentially realized by effectively harnessing thermal energy from various heat sources, e.g., the waste heat dissipated from nanoelectronic devices. These findings open up an avenue of thermal plasmonics based on graphene for different applications ranging from infrared emission control, to information processing and communication, to energy harvesting.", "desc": "thermal information communication via graphene surface plasmons", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 111, "fields": {"turk_id": "EKk8ZfFFGd", "abstract": "Automatic real-time captioning provides immediate and on demand access to spoken content in lectures or talks, and is a crucial accommodation for deaf and hard of hearing (DHH) people. However, in the presence of specialized content, like in technical talks, automatic speech recognition (ASR) still makes mistakes which may render the output incomprehensible. In this paper, we introduce a new approach, which allows audience or crowd workers, to quickly correct errors that they spot in ASR output. Prior approaches required the crowd worker to manually __edit__ the ASR hypothesis by selecting and replacing the text, which is not suitable for real-time scenarios. Our approach is faster and allows the worker to simply type corrections for misrecognized words as soon as he or she spots them. The system then finds the most likely position for the correction in the ASR output using keyword search (KWS) and stitches the word into the ASR output. Our work demonstrates the potential of computation to incorporate human input quickly enough to be usable in real-time scenarios, and may be a better method for providing this vital accommodation to DHH people.", "desc": "allows audience or crowd workers, to quickly correct errors that they spot in Automated Speech Recognition output", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 112, "fields": {"turk_id": "G427103nql", "abstract": "Basic social psychological research has suggested several interventions to reduce intergroup conflict. Most of these interventions, however, have been indirect and impractical to implement outside laboratory settings. Although past research has demonstrated that indirect manipulations of the consideration of future consequences reduce intergroup competition (Insko et al., 1998, 2001), no study of interindividual-intergroup discontinuity has tested this assumption with a direct manipulation. The present study found that when participants (individuals and members of groups) interacting in an iterated prisoners dilemma game were asked to predict how their opponents choice on a second trial would be affected by their own choice on an initial trial, intergroup competition was reduced while interindividual competition remained low regardless of the manipulation. On a practical level, implications of this study provide a simple and easily implemented solution to reducing intergroup conflict in non-laboratory situations.", "desc": "Reducing intergroup conflict", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 113, "fields": {"turk_id": "HOFETrsj4l", "abstract": "Biofuels have received legislative support recently in Californias Low-Carbon Fuel Standard and the Federal Energy Independence and Security Act. Both present new fuel types, but neither provides methodological guidelines for dealing with the inherent uncertainty in evaluating their potential lifecycle greenhouse gas emissions. Emissions reductions are based on point estimates only. This work demonstrates the use of Monte Carlo simulation to estimate life-cycle emissions distributions from ethanol and butanol from corn or switchgrass. Life-cycle emissions distributions for each feedstock and fuel pairing modeled span an order of magnitude or more. Using a streamlined life-cycle assessment, corn ethanol emissions range from 50 to 250 g CO2e/MJ, for example, and each feedstockfuel pathway studied shows some probability of greater emissions than a distribution for gasoline. Potential GHG emissions reductions from displacing fossil fuels with biofuels are difficult to forecast given this high degree of uncertainty inlife-cycle emissions. This uncertainty is driven by the importance and uncertainty of indirect land use change emissions. Incorporating uncertainty in the decision making process can illuminate the risks of policy failure (e.g., increased emissions), and a calculated risk of failure due to uncertainty can be used to inform more appropriate reduction targets in future biofuel policies.", "desc": "Uncertainty in evauating the potential lifecycle greenhouse gas emissions.", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 114, "fields": {"turk_id": "IAUazaTN5B", "abstract": "Building information models (BIMs) are maturing as a new paradigm for storing and exchanging knowledge about a facility. BIMs constructed from a CAD model do not generally capture details of a facility as it was actually built. Laser scanners can be used to capture dense 3D measurements of a facility's as-built condition and the resulting point cloud can be manually processed to create an as-built BIM  a time-consuming, subjective, and error-prone process that could benefit significantly from automation. This article surveys techniques developed in civil engineering and computer science that can be utilized to automate the process of creating as-built BIMs. We sub-divide the overall process into three core operations: geometric modeling, object recognition, and object relationship modeling. We survey the state-of-the-art methods for each operation and discuss their potential application to automated as-built BIM creation. We also outline the main methods used by these algorithms for representing knowledge about shape, identity, and relationships. In addition, we formalize the possible variations of the overall as-built BIM creation problem and outline performance evaluation measures for comparing as-built BIM creation algorithms and tracking progress of the field. Finally, we identify and discuss technology gaps that need to be addressed in future research.", "desc": "automate the process of creating as-built building information models.", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 115, "fields": {"turk_id": "Ifb7TXHY2L", "abstract": "Defects experienced during construction are costly and preventable. However, inspection programs employed today cannot adequately detect and manage defects that occur on construction sites, as they are based on measurements at specific locations and times, and are not integrated into complete electronic models. Emerging sensing technologies and project modeling capabilities motivate the development of a formalism that can be used for active quality control on construction sites. In this paper, we outline a process of acquiring and updating detailed design information, identifying inspection goals, inspection planning, as-built data acquisition and analysis, and defect detection and management. We discuss the validation of this formalism based on four case studies.", "desc": "Acquiring and updating detailed design information, goals, inspection planning, and defect detection and management.", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 116, "fields": {"turk_id": "JrI2r2Ul7c", "abstract": "Distant labeling for information extraction (IE) suffers from noisy training data. We describe a way of reducing the noise associated with distant IE by identifying coupling constraints between potential instance labels. As one example of coupling, items in a list are likely to have the same label. A second example of coupling comes from analysis of document structure: in some corpora, sections can be identified such that items in the same section are likely to have the same label. Such sections do not exist in all corpora, but we show that augmenting a large corpus with coupling constraints from even a small, well-structured corpus can improve performance substantially, doubling F1 on one task.", "desc": "reducing the noise associated with distant information extraction by identifying coupling constraints", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 117, "fields": {"turk_id": "KiLt4W3IoW", "abstract": "During the design process, a designer transforms an abstract functional description for a device into a physical description that satisfies the functional requirements. In this sense, design is a transformation from the functional domain to the physical domain; however, this transformation process is not well characterized nor understood for mechanical systems. The difficulty arises, at least in pan, because mechanical designs are often composed of highlyintegrated, tightly-coupled components where the interactions among the components are essential to the behavior and economic execution of the design. This assertion runs counter to design methodologies in other engineering fields, such as software design and circuit design, that result in designs in which each component fulfills a single function with minimal interaction. Because of the geometry, weight, and cost of mechanical components, converting a single behavioral requirement into a single component is often both impractical and infeasible. Each component may contribute to several required behaviors, and a single required system behavior may involve many components. In fact, most mechanical  components perform not only the desired behavior, but also many additional, unintended behaviors. In good mechanical designs, these additional behaviors often are exploited. The long term goal of our research is to create a transformational strategy in which the design specifications for a mechanical system can be transformed into a description of a collection of mechanical components. To realize this goal requires formal representations for the behavioral and the physical specifications of mechanical systems as well as formal representations for the behaviors and the physical characteristics of mechanical components. Because the interactions of components are important in our synthesis strategy, the representation of the behaviors of mechanical components must be . linked to the representation of their physical characteristics; that is, we are concerned with modeling the relationship between form and function of components. Finally, we need a strategy that enables us to transform an abstract description of the desired behavior of a > device into a description that corresponds to a collection of available physical components. In this paper, we present a graph-based language to describe both the behavioral specifications of a design as well as the behavior of the available physical components. We also briefly discuss a graphbased grammar for the representation of the physical characteristics of the components that enables us to guide the translation from specifications to components [Pinilla 89]. The transformation strategy is discussed in a companion paper [Hoover 89].", "desc": "create a transformational strategy in which the design specifications for a mechanical system can be transformed into a description of a collection of mechanical components", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 118, "fields": {"turk_id": "KzBuCYghRH", "abstract": "Fluorescence detection and imaging are vital technologies in the life sciences and clinical diagnostics. The key to obtaining high-resolution images and sensitive detection is to use fluorescent molecules or particles that absorb and emit visible light with high efficiency. We have synthesized supramolecular complexes consisting of a branched DNA template and fluorogenic intercalating dyes. Since dyes can intercalate up to every other base pair, high densities of fluorophores are assembled, yet the DNA template keeps them far enough away from each other to prevent self-quenching. The efficiency with which these noncovalent assemblies absorb light is more than 10-fold greater than that of the individual dye molecules. Frster resonance energy transfer (FRET) from the intercalated dyes to covalently attached acceptor dyes is very efficient, allowing for wavelength shifting of the emission spectrum. Simple biotinylation of the DNA template allows for labeling of streptavidin-coated synthetic microspheres and mouse T-cells.", "desc": "obtaining high-resolution images and sensitive detection in life sciences.", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 119, "fields": {"turk_id": "Lw1iylzF3m", "abstract": "In the early nineteenth century, Henry Brougham endeavored to improve the moral character of England through the publication of educational texts. Soon after, Brougham helped form the Society for the Diffusion of Useful Knowledge to carry his plan of moral improvement to the people. Despite its goal of improving the nation!s moral character, the Society refused to publish any treatises on explicitly moral or religious topics. Brougham instead turned to a mathematician, Augustus De Morgan, to promote mathematics as a rational subject that could provide the link between the secular and religious worlds. Using specific examples gleaned from the treatises of the Society, this article explores both how mathematics was intended to promote the development of reason and morality and how mathematical content was shaped to fit this particular view of the usefulness of mathematics. In the course of these treatises De Morgan proposed a fundamentally new pedagogical approach, one which focused on the student and the role mathematics could play in moral education.", "desc": "PROVIDE THE LINK BETWEEN THE SECULAR AND RELIGIOUS WORLDS THROUGH MATHEMATICS", "user": null, "trial": null, "type": "understand", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 120, "fields": {"turk_id": "MUzbr5zmwY", "abstract": "In this paper, we examine the use of spatial layouts of musical material for live performance control. Emphasis is given to software tools that provide for the simple and intuitive geometric organization of sound material, sound processing parameters, and higher-level musical structures.", "desc": "provide simple and intuitive geometric organization of sound material, sound processing parameters, and higher-level musical structures", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 121, "fields": {"turk_id": "MhjZpniqTf", "abstract": "In this paper, we investigate the patterns of design choices made by classroom teachers for decorating their classroom walls, using cluster analysis to see which design decisions go together. Classroom visual design has been previously studied, but not in terms of the systematic patterns adopted by teachers in selecting what materials to place on classroom walls, or in terms of the actual semantic content of what is placed on walls. This is potentially important, as classroom walls are continuously seen by students, and form a continual off-task behavior option, available to students at all times. Using the k-means clustering algorithm, we find four types of visual classroom environments (one of them an outlier within our data set), representing teachers strategies in classroom decoration. Our results indicate that the degree to which teachers place content-related decorations on the walls, is a feature of particular importance for distinguishing which approach teachers are using. Similarly, the type of school (e.g. whether private or charter) appeared to be another significant factor in determining teachers design choices for classroom walls. The present findings begin the groundwork to better understand the impact of teacher decisions and choices in classroom design that lead to better outcomes in terms of engagement and learning, and finally towards developing classroom designs that are more effective and engaging for learners.", "desc": "patterns of design choices made by classroom teachers.", "user": null, "trial": null, "type": "understand", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 122, "fields": {"turk_id": "NTOJFnXKGA", "abstract": "In this study, the authors used the Facial Action Coding System (FACS; P. Ekman & W. V. Friesen, 1978) to examine the immediate facial responses of abstinent smokers exposed to smoking cues. The aim was to investigate whether facial expressions thought to be linked to ambivalence would relate to more traditional measures of ambivalence about smoking. The authors adapted N. A. Heather's (1998) definition of ambivalence about smoking, which emphasizes difficulty in refraining from smoking despite intentions to do so. Ambivalence expressed during smoking cue exposure was operationalized as the simultaneous occurrence of positive and negative affect-related facial expressions. Thirty-four nicotine-deprived dependent smokers were presented with in vivo smoking cues, and their facial expressions were coded using FACS. Participants also completed self-report measures related to ambivalence about smoking. Smokers who displayed ambivalent facial expressions during smoking cue exposure reported significantly higher scores on measures of smoking ambivalence than did those who did not display ambivalent facial expressions.", "desc": "Immediate facial responses of abstinent smokers exposed to smoking cues.", "user": null, "trial": null, "type": "make??", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 123, "fields": {"turk_id": "NeLtw7bNBk", "abstract": "Ionic transition metal complexes (iTMCs) are receiving increased attention as materials capable of yielding efficient electroluminescent devices with air-stable electrodes. The operational characteristics of these devices are dominated by the presence of mobile ions that redistribute under an applied bias and assist in electronic charge injection. This article reviews recent efforts in the field of iTMC devices: i) to understand their physics, ii) to improve their efficiency, colour, turn-on time and lifetime, and iii) to expose their potential applications.", "desc": "Explore the use of Ionic transition metal complexes (iTMCs) for yielding efficient electroluminescent devices with air-stable electrodes.", "user": null, "trial": null, "type": "make + understand", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 124, "fields": {"turk_id": "O6DW7fz0md", "abstract": "Just as the movie Star Wars had a prequel, so did the DNA Warsthe series of legal, scientific, and personal battles that took place over the admissibility of forensic DNA evidence from 1989 to 1994. Between the late 1970s and the mid-1980s, another forensic identification technique became mired in controversy: electrophoresis-based blood protein analysis. Although the debates over blood analysis were every bit as rancorous and frustrating to almost everybody involvedso much so that they became known as the Starch Warstheir importance has not been adequately appreciated in the recent history of forensic science. After reviewing the early history of blood typing, I will describe the development of the Multi-System approach to blood protein analysis that took place in California from 1977 to 1978. I will then elucidate the history of the Starch Wars, and demonstrate the ways that they shaped subsequent disputes over DNA evidence, especially in California. I will show that: (a) many of the forensic scientists, law enforcement officials, and lawyers who became prominent players in the DNA Wars were deeply involved in the court cases involving protein electrophoresis; and (b) many of the issues that became controversial in the disputes over DNA evidence first emerged in the Starch Wars. In the conclusion, I will suggest various ways to improve the quality of forensic science based on my analysis of the Starch Wars.", "desc": "improve the quality of forensic science based on the analysis of the Starch Wars.", "user": null, "trial": null, "type": "understand", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 125, "fields": {"turk_id": "OndxCSPkAF", "abstract": "Large wall-sized displays are becoming prevalent. Although researchers have articulated qualitative benefits of group work on large displays, little work has been done to quantify the benefits for individual users. We ran two studies comparing the performance of users working on a large projected wall display to that of users working on a standard desktop monitor. In these studies, we held the visual angle constant by adjusting the viewing distance to each of the displays. Results from the first study indicate that although there was no significant difference in performance on a reading comprehension task, users performed about 26% better on a spatial orientation task done on the large display. Results from the second study suggest that the large display affords a greater sense of presence, allowing users to treat the spatial task as an egocentric rather than an exocentric rotation. We discuss future work to extend our findings and formulate design principles for computer interfaces and physical workspaces. Large wall-sized displays are becoming prevalent. Although researchers have articulated qualitative benefits of group work on large displays, little work has been done to quantify the benefits for individual users. We ran two studies comparing the performance of users working on a large projected wall display to that of users working on a standard desktop monitor. In these studies, we held the visual angle constant by adjusting the viewing distance to each of the displays. Results from the first study indicate that although there was no significant difference in performance on a reading comprehension task, users performed about 26% better on a spatial orientation task done on the large display. Results from the second study suggest that the large display affords a greater sense of presence, allowing users to treat the spatial task as an egocentric rather than an exocentric rotation. We discuss future work to extend our findings and formulate design principles for computer interfaces and physical workspaces.", "desc": "comparing the performance of users working on a large wall display to those who work on a standard desktop monitor", "user": null, "trial": null, "type": "understand", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 126, "fields": {"turk_id": "PCTOapDe2u", "abstract": "Large-scale content-based semantic search in video is an interesting and fundamental problem in multimedia analysis and retrieval. Existing methods index a video by the raw concept detection score that is dense and inconsistent, and thus cannot scale to \"big data\" that are readily available on the Internet. This paper proposes a scalable solution. The key is a novel step called concept adjustment that represents a video by a few salient and consistent concepts that can be efficiently indexed by the modified inverted index. The proposed adjustment model relies on a concise optimization framework with interpretations. The proposed index leverages the text-based inverted index for video retrieval. Experimental results validate the efficacy and the efficiency of the proposed method. The results show that our method can scale up the semantic search while maintaining state-of-the-art search performance. Specifically, the proposed method (with reranking) achieves the best result on the challenging TRECVID Multimedia Event Detection (MED) zero-example task. It only takes 0.2 second on a single CPU core to search a collection of 100 million Internet videos.", "desc": "large-scale content-based semantic search in videoes", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 127, "fields": {"turk_id": "R3cMkwnS7c", "abstract": "Large, chronically-implanted arrays of microelectrodes are an increasingly common tool for recording from primate cortex, and can provide extracellular recordings from many (order of 100) neurons. While the desire for cortically-based motor prostheses has helped drive their development, such arrays also offer great potential to advance basic neuroscience research. Here we discuss the utility of array recording for the study of neural dynamics. Neural activity often has dynamics beyond that driven directly by the stimulus. While governed by those dynamics, neural responses may nevertheless unfold differently for nominally identical trials, rendering many traditional analysis methods ineffective. We review recent studies  some employing simultaneous recording, some not  indicating that such variability is indeed present both during movement generation, and during the preceding premotor computations. In such cases, large-scale simultaneous recordings have the potential to provide an unprecedented view of neural dynamics at the level of single trials. However, this enterprise will depend not only on techniques for simultaneous recording, but also on the use and further development of analysis techniques that can appropriately reduce the dimensionality of the data, and allow visualization of single-trial neural behavior.", "desc": "recording from primate cortex", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 128, "fields": {"turk_id": "RAbj25EMJk", "abstract": "Learning key concepts in music theory, such as rhythm and harmony, pitch, and counterpoint, is a complex process for young learners. Problems encountered in building the perceptual knowledge of these concepts lie in a lack of flexibility in practicing these concepts without mastering a musical instrument. This paper describes NoteCubes, a set of tangible interactive construction toy blocks for children to explore and build understanding of musical concepts by creating simple melodies of their own based on the spatial reasoning with a playful interface. We also report initial user feedback from a small-scale exhibition and discuss future directions for further development and refinement of this tangible musical toy.", "desc": "to explore and build understanding of musical concepts by creating simple melodies based on the spatial reasoning with a playful interface", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 129, "fields": {"turk_id": "S8U4lCwufw", "abstract": "Managing power generation from wind is conceptually straightforward: generate and sell as much electricity as possible when prices are positive, and do nothing otherwise. However, this strategy leads to curtailment when wind energy exceeds the transmission capacity or when prices are negative, and possible revenue dilution when current prices are low but are expected to increase in the future. Electricity storage provides a means to alleviate these problems, and also enables the purchase of electricity from the market for later resale. But the presence of storage and transmission capacity complicates the management of electricity generation from wind. Much is unknown about the management of such a generation plus storage and transmission system, for instance, how complex the optimal management of such a system is, how valuable storage is, and how relevant it is to include different factorssuch as negative prices, the buying option, and future information. We answer these questions by developing and analyzing a Markov decision process model of this system.", "desc": "Generating and managing electricity from wind.", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 130, "fields": {"turk_id": "TR4csMMpeg", "abstract": "Many organizations look to carbon footprint protocols for guidance on measuring their greenhouse gas emissions, or carbon footprint. Existing protocols generally require estimation of direct emissions (Scope 1) and emissions from direct purchases of energy (Scope 2), but focus less on indirect emissions upstream and downstream of the supply chain (optional Scope 3). Because on average more than 75% of an industry sectors carbon footprint is attributed to Scope 3 sources, better knowledge of Scope 3 footprints can help organizations pursue emissions mitigation projects not just within their own plants but also across their supply chain. In this work, Scope 3 footprints of U.S. economic sectors are categorized using an Economic Input-Output Life Cycle Assessment (EIOLCA) model to identify upstream emission sources that are likely to contribute significantly to different sectors footprints. The portions of the upstream footprint captured by the sectors top-10 upstream suppliers are estimated at 3 different levels of specificity: general economy-wide, industry specific, and sector specific. The results show that enterprises can capture a large portion of their total upstream carbon footprint by collecting full emissions information from only a handful of direct suppliers, and Scope 3 footprint capture rates can be improved considerably by sector-specific categorization. Employee commuting and air transportation may be more important (7%-30%) for the services industries, but should not be a focus of detailed Scope 3 footprint estimates for the manufacturing industries (<1% of the total analyzed footprint). Protocol organizations should actively make more specific Scope 3 guidelines available for their constituents by developing sectorspecific categorizations for as many sectors as they feasibly can and create broader industry-specific protocols for others.", "desc": "identify upstream emission sources that are likely to contribute significantly to different sectors footprints", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 131, "fields": {"turk_id": "TjQwjq71Et", "abstract": "Neural prostheses translate neural activity from the brain into control signals for guiding prosthetic devices, such as computer cursors and robotic limbs, and thus offer disabled patients greater interaction with the world. However, relatively low performance remains a critical barrier to successful clinical translation; current neural prostheses are considerably slower with less accurate control than the native arm. Here we present a new control algorithm, the recalibrated feedback intention-trained Kalman filter (ReFIT-KF), that incorporates assumptions about the nature of closed loop neural prosthetic control. When tested with rhesus monkeys implanted with motor cortical electrode arrays, the ReFIT-KF algorithm outperforms existing neural prostheses in all measured domains and halves acquisition time. This control algorithm permits sustained uninterrupted use for hours and generalizes to more challenging tasks without retraining. Using this algorithm, we demonstrate repeatable high performance for years after implantation across two monkeys, thereby increasing the clinical viability of neural prostheses.", "desc": "relatively low performance remains a critical barrier to successful clinical translation of current neural prostheses", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 132, "fields": {"turk_id": "UdDy1PIFhK", "abstract": "OpenSound Control (OSC) is a protocol for communication among computers, sound synthesizers, and other multimedia devices that is optimized for modern networking technology. OSC has achieved wide use in the field of computer-based new interfaces for musical expression for wide-area and local-area networked distributed music systems, inter-process communication, and even within a single application.", "desc": "communicating among computers, sound synthesizers, and other multimedia devices", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 133, "fields": {"turk_id": "Usnh0z6cbX", "abstract": "Photolysis of water, a long-studied strategy for storing solar energy, involves two half-reactions: the reduction of protons to dihydrogen and the oxidation of water to dioxygen. Proton reduction is well-understood, with catalysts achieving quantum yields of 34% when driven by visible light. Water oxidation, on the other hand, is much less advanced, typically involving expensive metal centers and rarely working in conjunction with a photochemically powered system. Before further progress can be made in the field of water splitting, significant developments in the catalysis of oxygen evolution are needed. Herein we present an iron-centered tetraamido macrocyclic ligand (FeTAML) that efficiently catalyzes the oxidative conversion of water to dioxygen. When the catalyst is combined in unbuffered solution with ceric ammonium nitrate, its turnover frequency exceeds 1.3 s-1. Real-time UV-vis and oxygen monitoring of the active complex give insights into the reaction and decay kinetics.", "desc": "Catalyze oxidative conversion of water to dioxygen efficiently", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 134, "fields": {"turk_id": "X53CxW3Zxq", "abstract": "Prior research suggests people have trouble juggling effort across multiple projects with multiple partners. We investigated this problem, with an experiment where groups of four participants enacted the roles of police detectives. Each detective was assigned two homicide cases, each case with a different partner. To solve each case, detectives read their case documents and discussed relevant information with their partners. Half the groups used IM to communicate and the other half used an enhanced IM tool called Project-View IM (PVIM). PVIM lists partners and joint projects and lets users know what a partner is working on. We analyzed keystroke level computer activity and the content of conversations. Generally, work unfolded as follows: coordinate across cases, start first case, read documents, coordinate within case with partner, switch to second case, and so on, but with frequent interruptions. We describe implications of our findings for theories of multitasking.", "desc": "juggling effort across multiple projects with multiple partners", "user": null, "trial": null, "type": "understand", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 135, "fields": {"turk_id": "XnTJ6frLJF", "abstract": "Probabilistic decoding techniques have been used successfully to infer time-evolving physical state, such as arm trajectory or the path of a foraging rat, from neural data. A vital element of such decoders is the trajectory model, expressing knowledge about the statistical regularities of the movements. Unfortunately, trajectory models that both 1) accurately describe the movement statistics and 2) admit decoders with relatively low computational demands can be hard to construct. Simple models are computationally inexpensive, but often inaccurate. More complex models may gain accuracy, but at the expense of higher computational cost, hindering their use for real-time decoding. Here, we present a new general approach to defining trajectory models that simultaneously meets both requirements. The core idea is to combine simple trajectory models, each accurate within a limited regime of movement, in a probabilistic mixture of trajectory models (MTM). We demonstrate the utility of the approach by using an MTM decoder to infer goal-directed reaching movements to multiple discrete goals from multi-electrode neural data recorded in monkey motor and premotor cortex. Compared with decoders using simpler trajectory models, the MTM decoder reduced the decoding error by 38 (48) percent in two monkeys using 98 (99) units, without a necessary increase in running time. When available, prior information about the identity of the upcoming reach goal can be incorporated in a principled way, further reducing the decoding error by 20 (11) percent. Taken together, these advances should allow prosthetic cursors or limbs to be moved more accurately toward intended reach goals.", "desc": "infer time-evolving physical state from neural data", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 136, "fields": {"turk_id": "Xt2Gnk7MCQ", "abstract": "Rapid feedback is a core component of mastery learning, but feedback on open-ended work requires days or weeks in most classes today. This paper introduces PeerStudio, an assessment platform that leverages the large number of students' peers in online classes to enable rapid feedback on in-progress work. Students submit their draft, give rubric-based feedback on two peers' drafts, and then receive peer feedback. Students can integrate the feedback and repeat this process as often as they desire. In MOOC deployments, the median student received feedback in just twenty minutes. Rapid feedback on in-progress work improves course outcomes: in a controlled experiment, students' final grades improved when feedback was delivered quickly, but not if delayed by 24 hours. More than 3,600 students have used PeerStudio in eight classes, both massive and in-person. This research demonstrates how large classes can leverage their scale to encourage mastery through rapid feedback and revision.", "desc": "encourage mastery through rapid feedback and revision.", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 137, "fields": {"turk_id": "YcF1Lr6mHF", "abstract": "Robotic prostheses have the potential to significantly improve mobility for people with lower-limb amputation. Humans exhibit complex responses to mechanical interactions with these devices, however, and computational models are not yet able to predict such responses meaningfully. Experiments therefore play a critical role in development, but have been limited by the use of product-like prototypes, each requiring years of development and being specialized for a narrow range of functions. Here we describe a robotic ankle-foot prosthesis system that enables rapid exploration of a wide range of dynamical behaviors in experiments with human subjects. This emulator comprises powerful off-board motor and control hardware, a flexible Bowden cable tether, and a lightweight instrumented prosthesis, resulting in a combination of low mass worn by the human (0.96 kg) and high mechatronic performance compared to prior platforms. Benchtop tests demonstrated closed-loop bandwidth of 17 Hz, peak torque of 175 Nm and peak power of 1.0 kW. Tests with an anthropomorphic pendulum leg demonstrated low interference from the tether, less than 1 Nm about the hip. This combination of low worn mass, high bandwidth, and unrestricted movement make the platform exceptionally versatile. To demonstrate suitability for human experiments, we performed preliminary tests in which a subject with unilateral transtibial amputation walked on a treadmill at 1.25 ms _1 while the prosthesis behaved in various ways. These tests revealed low torque tracking error (RMS error of 2.8 Nm) and the capacity to systematically vary work production or absorption across a broad range (from -5 J to 21 J per step). These results support the use of robotic emulators during early-stage assessment of proposed device functionalities and for scientific study of fundamental aspects of human-robot interaction. The design of simple, alternate end-effectors would enable study at other joints or additional degrees of freedom.", "desc": "improve mobility for people with lower-limb amputation.", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 138, "fields": {"turk_id": "alrF9Z0FKN", "abstract": "Scheduling surgeries in hospitals is one of the most challenging activities for surgical staff. Schedule changes occur as often as every few moments, affecting necessary coordination of tasks, resources, and people within and across staff groups, and the stress people feel. In prior fieldwork at four sites, we observed that the physical layout of hallways and rooms, and barriers and spaces around schedule displays and key coordinators, affected information sharing and coordination of the surgery schedule. To generalize beyond the sites studied, we conducted a survey of 135 surgical suite directors across the USA. Our findings suggest how the architecture of the physical space and information availability and practices influence information sharing and coordination outcomes. Visual access between the shared surgery schedule display and the nursing control desk influenced whether staff groups congregated around schedule boards. Traffic-free areas around the surgery schedule display and up-to-date surgery schedule display information reduced coordination stress. We discuss implications for information practices and new information technology in hospital settings", "desc": "schedule surgeries in hospitals", "user": null, "trial": null, "type": "understand", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 139, "fields": {"turk_id": "bN1T4TZpwt", "abstract": "Techniques for three dimensional (3D) imaging and analysis of as-built conditions of buildings are gaining acceptance in the Architecture, Engineering, and Construction (AEC) community. Early detection of defects on construction sites is one domain where these techniques have the potential to revolutionize an industry, since construction defects can consume a significant portion of a projects budget. The ASDMCon project is developing methods to aid site managers in detecting and managing construction defects using 3D imaging and other advanced sensor technologies. This paper presents an overview of the project, its 4D visualization environment, and the 3D segmentation and recognition strategies that are being employed to automate defect detection.", "desc": "detecting and managing construction defects", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 140, "fields": {"turk_id": "dNI1C81GWr", "abstract": "The authors have built the first three-dimensional, kneed, two-legged, passive-dynamic walking machine. Since the work of Tad McGeer in the late 1980s, the concept of passive dynamics has added insight into animal locomotion and the design of anthropomorphic robots. Various analyses and machines that demonstrate efficient humanlike walking have been developed using this strategy. Human-like passive machines, however, have only operated in two dimensions (i.e., within the fore-aft or sagittal plane). Three-dimensional passive walking devices, mostly toys, have not had human-like motions but instead a stiff legged waddle. In the present three-dimensional device, the authors preserve features of McGeers two-dimensional models, including mechanical simplicity, human-like knee flexure, and passive gravitational power from descending a shallow slope. They then add specially curved feet, a compliant heel, and mechanically constrained arms to achieve a harmonious and stable gait. The device stands 85 cm tall. It weighs 4.8 kg, walks at about 0.51 m/s down a 3.1-degree slope, and consumes 1.3 W. This robot further implicates passive dynamics in human walking and may help point the way toward simple and efficient robots with human-like motions.", "desc": "The first three-dimensional, kneed, two-legged, passive-dynamic walking machine", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 141, "fields": {"turk_id": "e2f06an7eS", "abstract": "The concept of metabolism has been adopted from biology and refers to physiological processes within living things that provide the energy and nutrients required by an organism as the conditions of life itself. These processes can be described in terms of the transformation of inputs (sunlight, chemical energy, nutrients, water, and air) into biomass and waste products. While essentially a concept originating in science, I have found it useful as a means to comprehend the environmental history of cities. Just as living things require the inputs mentioned above, so do cities. That is, cities cannot exist without those inputsurbanites require clean air, water, food, fuel, and construction goods to subsist while urban industries need materials for production purposes. These materials may initially come from the area of the urban site itself, but increasingly over time they are derived from the urban hinterland or even farther. That is, as the city grows, it extends its ecological footprint deeper and deeper into its hinterland.", "desc": "Transforming cities' energy and nutrients into biomass and waste products.", "user": null, "trial": null, "type": "understand", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 142, "fields": {"turk_id": "ewOtP66RQo", "abstract": "The following will attempt clarify and complexify the notion of reflective practice. Ironically, invocations of reflective practice are often themselves not at all reflective of the consequences associated with where this practice comes from. (Ecclestone 1996) Ignorance of the genealogy of reflective practice risks the practice of reflective practice being insufficiently reflective. Awareness of that geneaology opens reflective practice to more effective techniques that are otherwise missed, whilst also limiting claims about the effectivity of reflective practice more generally. To put this another way, Donald Schon's notion of reflective practice is appealing because of its pragmatism. Developed from American pragmatist philosophies, John Dewey in particular, with the specific aim of capturing the pragmatism of design as a creative problem-solving expertise, reflective practice, as reflectionin-action. happens in and as the everyday of designing. For this reason, there is a danger of assuming that by merely being a designer one is always already a reflective practitioner, without having to act any differently. This is not just a misreading of Schon, but also a misreading of pragmatism, which I hope to show always has a romantic heart, by which I mean an element of affectively formative Willing - i.e., romanticism in the philosophic sense.' Reflective practices are how designers design, but they must also be designed, and, according to the philosophies grounding the notion of reflection, designed in ways that are more ambitious than the everyday pragmatics of designing.", "desc": "Clarify and complexify the notion of reflective practice.", "user": null, "trial": null, "type": "understand", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 143, "fields": {"turk_id": "fJZ0L9JilD", "abstract": "The generation and distribution of electricity comprises nearly 40% of U.S. CO2 emissions, as well as large shares of SO2, NOx, small particulates, and other toxins. Thus, correctly accounting for these electricity-related environmental releases is of great importance in life cycle assessment of products and processes. Unfortunately, there is no agreed-upon protocol for accounting for the environmental emissions associated with electricity, as well as significant uncertainty inthe estimates. Here, we explore the limits of current knowledge about grid electricity in LCA and carbon footprinting for the U.S. electrical grid, and show that differences in standards, protocols, and reporting organizations can lead to important differences in estimates of CO2, SO2, and NOx emissions factors. We find a considerable divergence in published values for grid emissions factor in the U.S. We discuss the implications of this divergence and list recommendations for a standardized approach to accounting for air pollution emissions in life cycle assessment and policy analyses in a world with incomplete and uncertain information.", "desc": "account for the environmental emissions associated with electricity correctly", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 144, "fields": {"turk_id": "gAwfSpWXau", "abstract": "The human visual system is faced with the computationally difficult problem of achieving object constancy: identifying three-dimensional (3D) objects via two-dimensional (2D) retinal images that may be altered when the same object is seen from different viewpoints1. A widely accepted class of theories holds that we first reconstruct a description of the object's 3D structure from the retinal image, then match this representation to a remembered structural description. If the same structural description is reconstructed from every possible view of an object, object constancy will be obtained. For example, in Biederman's2 oft-cited recognition-by-components (RBC) theory, structural descriptions are composed of sets of simple 3D volumes called geons (Fig. 1), along with the spatial relations in which the geons are placed. Thus a mug is represented in RBC as a noodle attached to the side of a cylinder, and a suitcase as a noodle attached to the top of a brick. The attraction of geons is that, unlike more complex objects, they possess a small set of defining properties that appear in their 2D projections when viewed from almost any position (e.g., all three views of the brick in Fig. 1 include a straight main axis, parallel edges, and a straight cross section). According to the RBC theory, a complex object can therefore be recognized from its constituent geons, which can themselves be recognized from any viewpoint.", "desc": "Achieving object constancy though viewing 3D objects by 2D retinal images at different viewpoints.", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 145, "fields": {"turk_id": "go0sG4eEJQ", "abstract": "This paper documents the development of a new instrument for the creation of experimental music theater. This environment, known as the liminal surface, uses a portable table-top design to integrate audio, video, analog and digital sensors, and computer-based control of external media (i.e. musical robotics). This environment will enable the composition of a series of new works exploring interactive computer music, intermodal relationships, and collaborative performance on a visually stimulating and technologically sophisticated platform.", "desc": "creation of experimental music theater", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 146, "fields": {"turk_id": "h6WOy7kSiP", "abstract": "This paper documents the development of Caress, an electroacoustic percussive instrument that blends drumming and audio synthesis in a small and portable form factor. Caress is an octophonic miniature drum-set for the fingertips that employs eight acoustically isolated piezo-microphones, coupled with eight independent signal chains that excite a unique resonance model with audio from the piezos. The hardware is designed to be robust and quickly reproducible (parametric design and machine fabrication), while the software aims to be light-weight (low-CPU requirements) and portable (multiple platforms, multiple computing architectures). Above all, the instrument aims for the level of control intimacy and tactile expressivity achieved by traditional acoustic percussive instruments, while leveraging real-time software synthesis and control to expand the sonic palette. This instrument as well as this document are dedicated to the memory of the late David Wessel, pioneering composer, performer, researcher, mentor and all-around Yoda of electroacoustic music.", "desc": "blend drumming and audio synthesis in a small and portable form factor", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 147, "fields": {"turk_id": "hDMxrtdIZc", "abstract": "This paper documents the development of ml.lib: a set of opensource tools designed for employing a wide range of machine learning techniques within two popular real-time programming environments, namely Max and Pure Data. ml.lib is a crossplatform, lightweight wrapper around Nick Gillians Gesture Recognition Toolkit, a C++ library that includes a wide range of data processing and machine learning techniques. ml.lib adapts these techniques for real-time use within popular dataflow IDEs, allowing instrument designers and performers to integrate robust learning, classification and mapping approaches within their existing workflows. ml.lib has been carefully designed to allow users to experiment with and incorporate machine learning techniques within an interactive arts context with minimal prior knowledge. A simple, logical and consistent, scalable interface has been provided across over sixteen externals in order to maximize learnability and discoverability. A focus on portability and maintainability has enabled ml.lib to support a range of computing architecturesincluding ARM and operating systems such as Mac OS, GNU/Linux and Windows, making it the most comprehensive machine learning implementation available for Max and Pure Data.", "desc": "Adapt techniques for real-time use within popular dataflow IDEs", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 148, "fields": {"turk_id": "hl53HHSrXk", "abstract": "This paper presents a new approach that allows automated recognition of three-dimensional (3D) ComputerAided Design (CAD) objects from 3D site laser scans. This approach provides a robust and efficient means to recognize objects in a scene by integrating planning technologies, such as multi-dimensional CAD modeling, and field technologies, such as 3D laser scanning. Using such an approach, it would be possible to visualize the 3D status of a project and automate some tasks related to project control. These tasks include: 3D progress tracking, productivity tracking, and construction dimensional quality assessment and quality control (QA/QC). This paper provides an overview of the developed approach, and demonstrates its performance in object recognition and project 3D status visualization, with data collected from a construction job site. Keywords: 3D, object recognition, automation, performance, visualization, project control", "desc": "automated recognition of three-dimensional (3D) ComputerAided Design (CAD) objects from 3D site laser scans", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 149, "fields": {"turk_id": "iIg1i1Nhwz", "abstract": "This presentation works from the assumption that the design of the coming decades, in the name of developing our societies sustainability, will involve realising less materials-intensive ways of living and working (dematerialisation). Designing such sustainable product-service systems requires a quite different approach to designing than that which is prevalent today. In this paper, I explore: 1. the extent to which designing, of any specialism, and especially as it is taught in universities, continues to remain wedded to making things, that is, to techn as the know-how of manufacturing finished products 2. the extent to which dematerialisation design involves something that can perhaps no longer be called a techn, less because its output is not a product, than because its output is not something that is never finished Drawing on Martin Heideggers accounts of the Ancient Greek productivism that continues to inform modern designing, I argue that the design of more sustainable product-service systems will need to pay greater heed to how things change over time. Since designers tend to exemplify Marxist theories of alienated labour with their desire to create once-and-for-alls, something like extended-designer-responsibility is needed, where designers are required to engage with their output beyond its production and sale/use. As an epilogue, the paper discusses the way design students invest heavily in the production of finished products for assessment, but then invariably never pick them up, satisfied merely with the intangible mark they receive. How could this situation be exploited to educe designers of sustainable systems rather than technicians more unsustainable stuff?", "desc": "realising less materials-intensive ways of living and working", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 150, "fields": {"turk_id": "jwD6QsPsPB", "abstract": "This review examines how advances in neuroscience are affecting civil law, criminal law, and law enforcement. Brain imaging techniques have already been used to detect brain injury, assess pain, and determine mental state and capacity for rational thought. There is also much excitement about using neuroimaging to detect lies and deception in legal and national security contexts. Despite claims of neuroimaging's revolutionary nature, numerous questions should be answered about their validity and reliability before they become widely adopted. Neuroscientists still do not fully understand the link between brain activity and behavior or memory formation. Important legal and ethical questions remain unresolved, particularly around the potential effect on juries and judges of colorful, but scientifically unproven, brain images. Finally, the very impetus behind the use of neuroscience in the legal systemto avoid the subjectivity and uncertainty of more traditional methods for assessing thought and behaviormay be misguided.", "desc": "examination of how advances in neuroscience are affecting civil law, criminal law, and law enforcement", "user": null, "trial": null, "type": "understand", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 151, "fields": {"turk_id": "nRH06D70mx", "abstract": "This study developed an original instrument that measures pragmatic comprehension in Japanese as a foreign language (JFL). It examined the ability to comprehend implied meaning encoded in conventional and nonconventional features and the effect of proficiency on comprehension. There were 63 college students of Japanese at 2 proficiency levels who completed a listening test measuring their ability to comprehend 3 types of implied meaning: indirect refusals, conventional indirect opinions, and nonconventional indirect opinions. Comprehension was analyzed for accuracy (scores on a multiple-choice measure) and comprehension speed (average time taken to answer items correctly). There was a significant effect of item type on accuracy but not on comprehension speed. A proficiency effect was observed on accuracy but not on comprehension speed. Analyses of error data and introspective verbal reports revealed the nature of comprehension difficulty among JFL learners.", "desc": "measure pragmatic comprehension in Japanese as a foreign language.", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 152, "fields": {"turk_id": "nsfFK9rqcZ", "abstract": "This study examined development of pragmatic comprehension ability across time. Twenty native speakers and 92 Japanese college students of English completed a computerized listening task measuring ability to comprehend two types of implied meaning in dialogues: indirect refusals (k = 24) and indirect opinions (k = 24). The participants comprehension was analyzed for accuracy (scores on the listening task) and comprehension speed (average time taken to answer each item correctly). L2 learners accuracy and comprehension speed improved significantly over a 7-week period. However, the magnitude of effect was lower for comprehension speed than for accuracy. This study also examined the relationships among general L2 proficiency (measured on the ITP TOEFL), speed of lexical judgment (measured on a word recognition task), and pragmatic comprehension ability. There was a significant relationship between proficiency and accuracy (r = 0.39), as well as between lexical access speed and comprehension speed (r = 0.40). However, L2 proficiency bore no relationship to comprehension speed, and lexical access speed had no relationship with accuracy. Moreover, accuracy and comprehension speed were not related to each other. These findings suggest that development of pragmatic knowledge and processing capacity of using the knowledge may not coincide perfectly in L2 development.", "desc": "development of pragmatic comprehension ability across time", "user": null, "trial": null, "type": "understand", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 153, "fields": {"turk_id": "qhHhT8bupU", "abstract": "This work is based on the recognition that there will always be a need for different representations of the same entity, albeit a building or building part, a shape or other complex attribute. This exigency ensues, formally, to define the relations between alternative representations, in order to support translation and identify where exact translation is possible, and to define coverage of different representations. We consider an abstraction of representations to model sorts that allows us to define algebraic operations on sorts and recognize algebraic relationships between sorts, providing us with a method for the analysis of representations, and the comparison of their coverage. We present the basis of support for a multi-representational environment.", "desc": "define the relations between alternate representations of the same entity", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 154, "fields": {"turk_id": "r210UmRerh", "abstract": "We are exploring the use of formal grammars to represent two distinct but interconnected attributes of mechanical designs: geometry and behavior. By creating a formal description of a limited set of behaviors for mechanical designs and a corresponding description of physical components, we can generate the description of a physical system that takes advantage of the multiple behaviors of its components. Our approach is based on the following assertions:  The behavioral requirements of mechanical systems can be represented using a graph grammar.  The behavioral characteristics of components can be represented using a graph grammar.  The physical characteristics of designs and components can be represented using formal topological and geometric models.  The behavioral and physical graphs of components can be linked parametrically or algorithmically.  The behavioral specifications graph can be formally transformed into a description of a physical system with associated behavioral and geometric . representations. The goal of our research is to create a transformational strategy by which the design specifications for a mechanical system can be transformed into a description of a collection of mechanical components. For any given design specification, many different physical systems could potentially meet that specification. Hence there are potentially many transformations that could be applied to a particular design specification. We are interested in those transformations of the design specifications which preserve the original behavior of the design. In this paper, we explore a set of transformations that enable us to move from specifications to topological configurations and from topological configurations to geometric configurations.", "desc": "exploring the use of formal grammars to represent two distinct but interconnected attributes of mechanical designs: geometry and behavior.", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 155, "fields": {"turk_id": "sNR3IffuIp", "abstract": "We consider a model for managing a single stage that produces multiple items. The production rates are nite and there are switchover times. The interarrival times and quantities of demands for the items are random, and demand may occur for a set of items. We consider order focussed measures: cost based on response times, service levels based on quoted lead times and Type-1 service. We operate the stage in the following manner: (1) There is a cyclic schedule that determines the sequence of items and the number of times a particular item is produced in a cycle; (2) Given a cyclic schedule, production of each item follows a modi ed base-stock policy or a (s,S) policy. We present a simulation based procedure to obtain good values for the base stock levels or S (for any xed S-s) for each of the above performance measures. Numerical results indicate that good solutions can be obtained with modest computational e ort. We also report on a real world implementation of this model.", "desc": "managing a single stage that produces multiple items", "user": null, "trial": null, "type": "mae", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 156, "fields": {"turk_id": "uhHBwhrmYM", "abstract": "We demonstrate an improvement in the turn-on time of electroluminescent devices based on the iridium complex [Ir(ppy)2(dtb-bpy)]+(PF6 -), where ppy is 2-phenylpyridine and dtb-bpy is 4,4_-di-tert-butyl- 2,2_-dipyridine, by introduction of the ionic liquid 1-butyl-3-methylimidazolium hexafluorophosphate BMIM+(PF6 -). Addition of 0.46 mol of the ionic liquid per mole of Ir complex reduces the turn-on time from 5 h to 40 min. However, the device lifetime is also reduced by a factor of 3 over this range, suggesting a tradeoff between device speed and stability. These results are discussed within the framework of the electrodynamic model of device operation and are found to be consistent with an increase in the ionic conductivity of the [Ir(ppy)2(dtb-bpy)]+(PF6 -) films upon the addition of ionic liquid.", "desc": "Improve turn-on time of electroluminescent devices", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 157, "fields": {"turk_id": "us20nxr2I6", "abstract": "We examine the possibility of distributed sensemaking: improving a users sensemaking by leveraging previous users work without those users directly collaborating or even knowing one another. We asked users to engage in sensemaking by organizing and annotating web search results into knowledge maps, either with or without previous users maps to work from. We also recorded gaze patterns as users examined others knowledge maps. Our findings show the conditions under which distributed sensemaking can improve sensemaking quality; that a users sensemaking process is readily apparent to a subsequent user via a knowledge map; and that the organization of content was more useful to subsequent users than the content itself, especially when those users had differing goals. We discuss the role distributed sensemaking can play in schema induction by helping users make a mental model of an information space and make recommendations for new tool and system development.", "desc": "improve a users sensemaking by leveraging previous users work without those users directly collaborating or even knowing one another", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 158, "fields": {"turk_id": "xBsmW8PFQi", "abstract": "We introduce an algorithm for automatic selection of semantically-resonant colors to represent data (e.g., using blue for data about __oceans__, or pink for __love__). Given a set of categorical values and a target color palette, our algorithm matches each data value with a unique color. Values are mapped to colors by collecting representative images, analyzing image color distributions to determine value-color affinity scores, and choosing an optimal assignment. Our affinity score balances the probability of a color with how well it discriminates among data values. A controlled study shows that expert-chosen semantically-resonant colors improve speed on chart reading tasks compared to a standard palette, and that our algorithm selects colors that lead to similar gains. A second study verifies that our algorithm effectively selects colors across a variety of data categories.", "desc": "automatic selection of semantically-resonant colors to represent data", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 159, "fields": {"turk_id": "y7mPut9ST1", "abstract": "We investigate physical properties that can be used to distinguish the valley degree of freedom in systems where inversion symmetry is broken, using graphene systems as examples. We show that the pseudospin associated with the valley index of carriers has an intrinsic magnetic moment, in close analogy with the Bohr magneton for the electron spin. There is also a valley dependent Berry phase effect that can result in a valley contrasting Hall transport, with carriers in different valleys turning into opposite directions transverse to an in-plane electric field. These effects can be used to generate and detect valley polarization by magnetic and electric means, forming the basis for the so-called valley-tronics applications.", "desc": "investigate physical properties that can be used to distinguish the valley degree of freedom in systems where inversion symmetry is broken.", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 160, "fields": {"turk_id": "yTIr5ey5q9", "abstract": "We present a novel use of the OpenSound Control (OSC) protocol to represent the output of gestural controllers as well as the input to sound synthesis processes. With this scheme, the problem of mapping gestural input into sound synthesis control becomes a simple translation from OSC messages into other OSC messages. We provide examples of this strategy and show benefits including increased encapsulation and program clarity.", "desc": "represent the output of gestural controllers as well as the input to sound synthesis processes", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 161, "fields": {"turk_id": "yieLwlO8O8", "abstract": "We present a primal-dual interior-point algorithm with a filter line-search method for nonlinear programming. Local and global convergence properties of this method were analyzed in previous work. Here we provide a comprehensive description of the algorithm, including the feasibility restoration phase for the filter method, second-order corrections, and inertia correction of the KKT matrix. Heuristics are also considered that allow faster performance. This method has been implemented in the IPOPT code, which we demonstrate in a detailed numerical study based on 954 problems from the CUTEr test set. An evaluation is made of several line-search options, and a comparison is provided with two state-of-the-art interior-point codes for nonlinear programming.", "desc": "primal-dual interior-point algorithm with a filter line-search method for nonlinear programming", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 162, "fields": {"turk_id": "zhGZ3z5KEm", "abstract": "We present an abstraction of representational schema to model sorts that allows us to explore the mathematical properties of a constructive approach to sorts. We apply this approach to representational schema defined as compositions of primitive data types, and explore a comparison of representational structures with respect to scope and coverage. We consider a behavioral specification for sorts in order to empower these representational structures to support design activities effectively. We provide an example of the use of sorts to represent alternative views to a design problem. We conclude with a comparison with other approaches for flexibility of design representations.", "desc": "Present schema to model mathematical properties of representational structures of primitive data types", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 163, "fields": {"turk_id": "zinyAFifli", "abstract": "We present an approach for the detection of coordinate-term relationships between entities from the software domain, that refer to Java classes. Usually, relations are found by examining corpus statistics associated with text entities. In some technical domains, however, we have access to additional information about the real-world objects named by the entities, suggesting that coupling information about the \"grounded\" entities with corpus statistics might lead to improved methods for relation discovery. To this end, we develop a similarity measure for Java classes using distributional information about how they are used in software, which we combine with corpus statistics on the distribution of contexts in which the classes appear in text. Using our approach, cross-validation accuracy on this dataset can be improved dramatically, from around 60% to 88%. Human labeling results show that our classifier has an F1 score of 86% over the top 1000 predicted pairs.", "desc": "Detect coordinate-term relationships between entities from the software domain", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}, {"model": "wordclouds.problem", "pk": 164, "fields": {"turk_id": "znfdmFhUWb", "abstract": "We present QueryViz, a novel visualization tool for SQL queries that reduces the time needed to read and understand existing queries. It targets two principal audiences: (i) users who often issue the same or similar queries and who need to quickly browse through a repository of existing queries; and (ii) novices that try to familiarize themselves with the logic behind alternative patterns of SQL queries. QueryViz uses as input only two strings: the database schema and the SQL query. It can thus serve as light-weight add-on to existing database systems and is also available via an online interface at http://queryviz.com. In this demonstration, we explain our visual alphabet, walk through the visualization algorithm, and let users experience the difference in understanding SQL queries from text or our graphical representation while browsing through repositories of well-known textbook SQL queries.", "desc": "Reduce the time needed to read and understand existing SQL queries.", "user": null, "trial": null, "type": "make", "completions": 0, "submit_date": "2016-07-19T16:59:41Z"}}]
